{
  "articles": [
    {
      "path": "about.html",
      "title": "About Me",
      "author": [],
      "contents": "\nMichael Weylandt is currently an Intelligence Community Postdoctoral Fellow, working with George Michailidis at the University of Florida. His work focuses on statistical machine learning theory for structured multivariate time series, viewed through a graphical model lens. His work has been recognized with best paper awards from the American Statistical Association in both Statistical Learning and Data Science and in Business & Economic Statistics. He has served as a mentor in the Google Summer of Code program for 7 years on behalf of the R Foundation for Statistical Computing and previously held an NSF Graduate Research Fellowship. Prior to beginning his Ph.D. studies, he worked at Morgan Stanley as a quantitative analyst, focusing on derivatives pricing and financial risk management. He received a Bachelor’s of Science in Engineering from Princeton University in 2008 and a Ph.D. in Statistics from Rice University in 2020.\nFor more details, see my Full CV.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:53-06:00"
    },
    {
      "path": "contact.html",
      "title": "Contact Information",
      "author": [],
      "contents": "\nLike most folks, I continue to work remotely most days. If you want to get in touch, please email me at michael.weylandt@ufl.edu or give me a call at 713-204-9366.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:54-06:00"
    },
    {
      "path": "index.html",
      "title": "Welcome!",
      "author": [],
      "contents": "\nHello and welcome to my website! My name is Michael Weylandt. I am currently an US Intelligence Community Postdoctoral Research Fellow hosted at the University of Florida Informatics Institute, where I am advised by George Michailidis as well as two mentors from the Intelligence Community. Before this, I earned my Ph.D. at Rice University in 2020, worked in finance from 2013 to 2015, and taught at Sherborne School in the UK from 2012 to 2013. My research is in statistical machine learning, with a recent focus on network data and in machine learning fairness. For a more formal bio, please see my CV and my About page. More detail on my Publications and Software can be found on those pages.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:54-06:00"
    },
    {
      "path": "publications/ccpd.html",
      "title": "Coupled-CP Decomposition for Principal Components Analysis of Symmetric Networks",
      "description": "Submitted article (under review)",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "George Michailidis",
          "url": {}
        }
      ],
      "contents": "\nAbstract: In a number of application domains, one observes a sequence of network data; for example, repeated measurements between users interactions in social media platforms, financial correlation networks over time, or across subjects, as in multi-subject studies of brain connectivity. One way to analyze such data is by stacking networks into a third-order array or tensor. We propose a principal components analysis (PCA) framework for sequence network data, based on a novel decomposition for semi-symmetric tensors. We derive efficient algorithms for computing our proposed “Coupled CP” decomposition and establish estimation consistency of our approach under an analogue of the spiked covariance model with rates the same as the matrix case up to a logarithmic term. Our framework inherits many of the strengths of classical PCA and is suitable for a wide range of unsupervised learning tasks, including identifying principal networks, isolating meaningful changepoints or outliers across observations, and for characterizing the “variability network” of the most varying edges. Finally, we demonstrate the effectiveness of our proposal on simulated data and on examples from political science and financial economics. The proof techniques used to establish our main consistency results are surprisingly straight-forward and may find use in a variety of other matrix and tensor decomposition problems.\nWorking Copy: ArXiv 2202.04719\nSummary: We develop and approach for performing Principal Components Analysis (PCA) on network series data - sets of networks observed on the same node set. Network series data are observed when a changing network is observed over time (e.g., a social media network at the end of each week) or when a statistical network is estimated from time series data (e.g., stock market correlation networks in different years). We approach this problem by embedding the network series in a semi-symmetric tensor and performing tensor PCA on this representation.\nSchematic Diagram of Semi-Symmetric Tensor PCAWe rigorously analyze Tensor PCA in the semi-symmetric context, proving consistency under an analogue of the “low-rank + noise” model for matrix PCA: somewhat remarkably, despite the difficulty of the tensor setting, our results are within a logarithmic factor of classical PCA results. Our proof technique depends on the classical Davis-Kahan theorem (and some painful algebra) and we hope to apply it further to a variety of tensor decomposition problems. Finally, we apply our method to a variety of synthetic and real data sets and find some counter-intuitive results about the Supreme Court of the United States.\nTensor PCA applied to SCOTUS Voting Behaviors\n\n\n",
      "last_modified": "2022-02-15T22:48:54-06:00"
    },
    {
      "path": "publications/clustRviz.html",
      "title": "Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization",
      "description": "*Journal of Computational and Graphical Statistics* 29(1), pp. 87-96. 2020",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "John Nagorski",
          "url": {}
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        }
      ],
      "contents": "\nWinner of the ASA Section on Statistical Learning and Data Science (SLDS) 2019 Student Paper Competition\nAbstract: Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at https://github.com/DataSlingers/clustRviz.\nPublisher DOI: 10.1080/10618600.2019.1629943\nPubMed: 32982130\nWorking Copy: ArXiv 1901.01477\nSummary:\nRelated Software: The clustering methodology from this paper is implemented in my R package clustRviz. The algorithmic regularization scheme from this paper was applied to the Generalized ADMM from my paper on co-clustering algorithms to give the improved CBASS algorithm in the package.\nCitation:\n@ARTICLE{Weylandt:2020,\n  AUTHOR=\"Michael Weylandt and John Nagorski and Genevera I. Allen\",\n  TITLE=\"Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization\",\n  JOURNAL=\"Journal of Computational and Graphical Statistics\",\n  YEAR=2020,\n  VOLUME=29,\n  NUMBER=1,\n  PAGES={87-96},\n  DOI=\"10.1080/10618600.2019.1629943\",\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:54-06:00"
    },
    {
      "path": "publications/coclustering_algs.html",
      "title": "Splitting Methods for Convex Bi-Clustering and Co-Clustering",
      "description": "*DSW 2019: Proceedings of the IEEE Data Science Workshop 2019*, pp.237-244. 2019.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Co-Clustering, the problem of simultaneously identifying clusters across multiple aspects of a data set, is a natural generalization of clustering to higher-order structured data. Recent convex formulations of bi-clustering and tensor co-clustering, which shrink estimated centroids together using a convex fusion penalty, allow for global optimality guarantees and precise theoretical analysis, but their computational properties have been less well studied. In this note, we present three efficient operator-splitting methods for the convex co-clustering problem: a standard two-block ADMM, a Generalized ADMM which avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests, and experimental evidence confirms, that the Generalized ADMM is far more efficient for large problems.\nPublisher DOI: 10.1109/DSW.2019.8755599\nWorking Copy: ArXiv 1901.06075\nSummary: Chi and Lange (J. Computational and Graphical Statistics, 2015) established that splitting methods are an easy-to-implement highly-performant way to solve convex clustering problems. In this work, I extend their analysis to the general case of tensor co-clustering, considering three operator-splitting schemes:\nThe classic ADMM\nA generalized (quadratically perturbed) ADMM\nDavis-Yin splitting (equivalent to AMA for this problem)\nClassical ADMM has the best per iteration performance, but requires solving a (tensor) Sylvester equation at each iteration, which is typically prohibitive. To avoid this, I construct a generalized ADMM scheme that avoids any matrix decompositions or inverses in the updates.\n\\[\\begin{align*}\n  \\mathbf{U}^{(k+1)} &= \\left(\\alpha \\mathbf{U}^{(k)} + \\mathbf{X} + \\rho\\mathbf{D}_{\\text{row}}^T(\\mathbf{V}^{(k)} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}} - \\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k)}) \\right. \\\\ &\\left.\\qquad+ \\rho(\\mathbf{V}^{(k)}_{\\text{col}} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}} - \\mathbf{U}^{(k)}\\mathbf{D}_{\\text{col}})\\mathbf{D}_{\\text{col}}^T \\right)/(1+\\alpha) \\\\\n\\begin{pmatrix} \\mathbf{V}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{V}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{row}, q}}(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}}) \\\\\n  \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{col}, q}}(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}})  \\end{pmatrix} \\\\\n    \\begin{pmatrix} \\mathbf{Z}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{Z}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\mathbf{Z}^{(k)}_{\\text{row}} + \\rho(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} - \\mathbf{V}^{(k+1)}_{\\text{row}}) \\\\ \\mathbf{Z}^{(k)}_{\\text{col}} + \\rho(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} - \\mathbf{V}^{(k+1)}_{\\text{col}})\n\\end{pmatrix}\n\\end{align*}\\] This method has slightly worse per iteration performance, but has superior wall clock performance and better computational complexity.\nPer Iteration PerformanceWall Clock PerformanceThe generalized ADMM is used internally in my clustRviz software for both the exact solver and the CBASS pathwise algorithm.\nRelated Software: clustRviz\nCitation:\n@INPROCEEDINGS{Weylandt:2019-BiClustering,\n  TITLE=\"Splitting Methods for Convex Bi-Clustering and Co-Clustering\",\n  AUTHOR=\"Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755599\",\n  PAGES={237-244}\n}\n\n@PROCEEDINGS{DSW:2019,\n  BOOKTITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:54-06:00"
    },
    {
      "path": "publications/conga.html",
      "title": "Sparse Partial Least Squares for Coarse Noisy Graph Alignment",
      "description": "*SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop*, pp.561-565. 2021.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "George Michailidis",
          "url": {}
        },
        {
          "name": "Thomas M. (\"Mitch\") Roddenberry",
          "url": "https://www.roddenberry.xyz/"
        }
      ],
      "contents": "\nAbstract: Graph signal processing (GSP) provides a powerful framework for analyzing signals arising in a variety of domains. In many applications of GSP, multiple network structures are available, each of which captures different aspects of the same underlying phenomenon. To integrate these different data sources, graph alignment techniques attempt to find the best correspondence between vertices of two graphs. We consider a generalization of this problem, where there is no natural one-to-one mapping between vertices, but where there is correspondence between the community structures of each graph. Because we seek to learn structure at this higher community level, we refer to this problem as “coarse” graph alignment. To this end, we propose a novel regularized partial least squares method which both incorporates the observed graph structures and imposes sparsity in order to reflect the underlying block community structure. We provide efficient algorithms for our method and demonstrate its effectiveness in simulations.\nPublisher DOI: 10.1109/SSP49050.2021.9513753\nWorking Copy: ArXiv 2104.02810\nSummary: We consider the problem of aligning two graphs on different node sets, e.g., Facebook and Twitter. Many users may be represented on both networks, but there are also a large number of users who use only one social media platform. Solving this problem exactly entails a (generalization) of the NP-hard graph-alignment problem, so we consider a relaxation in which we seek to find corresponding communities (e.g., basset hound enthusiasts) across both networks. To do so, we assume that common signals are observed on both networks (e.g., interest in a particular celebrity dog) and use that signal to align communities. Because this approach aligns communities rather than nodes and does so using (potentially noisy) signals, we refer to the resulting problem as the CONGA problem (Coarse Noisy Graph Alignment).\nCONGA Generative Model: Common Signals Excite Corresponding CommunitiesWe proceed by constructing the signal correlation matrix across the two graphs and performing a regularized SVD on this inner product:\n\\[\\begin{align*}\n    \\text{arg max}_{\\mathbf{U}, \\mathbf{V}} & \\text{Tr}(\\mathbf{U}^{\\top}\\mathbf{X}_1^\\top\\mathbf{X}_2\\mathbf{V}) - \\lambda_1P_1(\\mathbf{U}) - \\lambda_2P_2(\\mathbf{V})  \\\\ \n    \\text{subject to }&{\\mathbf{U} \\in \\textsf{conv}\\,\\mathcal{V}^{\\mathbf{I}_{n_1} + \\alpha_1\\mathbf{L}_1}_{n_1 \\times K}, \\mathbf{V} \\in \\textsf{conv}\\,\\mathcal{V}^{\\mathbf{I}_{n_2} + \\alpha_2\\mathbf{L}_2}_{n_2 \\times K}}\n\\end{align*}\\]\nThis approach builds on my work on multi-rank sparse and functional PCA and extends it to the partial least squares context. The use of the flexible SFPCA regularization framework allows us to simultaneously achieve four goals:\nEach community is supported on a sparse set of graph 1 nodes (\\(P_1(\\mathbf{U})\\))\nEach community is supported on a sparse set of graph 2 nodes (\\(P_2(\\mathbf{V})\\))\nEach community is well clustered with respect to graph 1, i.e., each community is smooth with respect to the Laplacian \\(\\mathbf{L}_1\\)\nEach community is well clustered with respect to graph 2, i.e., each community is smooth with respect to the Laplacian \\(\\mathbf{L}_2\\)\nThe resulting estimator is computationally efficient and performs well in simulation, as shown in our paper. We hope to further study this model theoretically, but correlated sampling of graphs on different node sets is a tricky and open problem. If you have particular expertise in this area, please do get in touch!\nPresentations: I presented on this work at SSP 2021, which was held virtually. A recording of my presentation can be found below:\n\n\nView on YouTube. Slides from this talk can be found here.\nCitation:\n@INPROCEEDINGS{Weylandt:2021-CONGA,\n  AUTHOR=\"Michael Weylandt and George Michailidis and Thomas M. Roddenberry\",\n  TITLE=\"Sparse Partial Least Squares for Coarse Noisy Graph Alignment\",\n  DOI=\"10.1109/SSP49050.2021.9513753\"\n  PAGES={561-565},\n  CROSSREF=\"SSP:2021\"\n}\n\n@PROCEEDINGS{SSP:2021,\n  BOOKTITLE=\"SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop\",\n  YEAR=2021,\n  LOCATION=\"Rio de Janeiro, Brazil\"\n  EDITOR=\"Jose Carlos Moreira Bermudez and Vitor H. Nascimento\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:55-06:00"
    },
    {
      "path": "publications/convex_wavelet_clustering.html",
      "title": "Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering",
      "description": "*DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021*, pp.1-8. 2021.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "Thomas M. (\"Mitch\") Roddenberry",
          "url": "https://www.roddenberry.xyz/"
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        }
      ],
      "contents": "\nAbstract: Clustering is a ubiquitous problem in data science and signal processing. In many applications where we observe noisy signals, it is common practice to first denoise the data, perhaps using wavelet denoising, and then to apply a clustering algorithm. In this paper, we develop a sparse convex wavelet clustering approach that simultaneously denoises and discovers groups. Our approach utilizes convex fusion penalties to achieve agglomeration and group-sparse penalties to denoise through sparsity in the wavelet domain. In contrast to common practice which denoises then clusters, our method is a unified, convex approach that performs both simultaneously. Our method yields denoised (wavelet-sparse) cluster centroids that both improve interpretability and data compression. We demonstrate our method on synthetic examples and in an application to NMR spectroscopy.\nPublisher DOI: 10.1109/DSLW51110.2021.9523413\nWorking Copy: ArXiv 2012.04762\nSummary: We consider the problem of clustering highly noisy signals. The dual nature of this problem gives rise to a puzzle: if we denoise the individual signals and then cluster, the resulting cluster centroids will be noisy, but if we cluster and then denoise, our clustering accuracy will be sub-par. We instead propose to simultaneous denoise and to cluster by combining wavelet soft-thresholding with convex clustering. The resulting approach identifies clusters which are denoised (wavelet sparse), while retaining the excellent statistical performance of convex clustering:\n Our approach be expressed as the following convex optimization problem:\n\\[\\frac{1}{2} \\|\\mathbf{U} - \\mathbf{X}\\|_F^2 + \\lambda  \\sum_{\\substack{i, j = 1 \\\\ i < j}}^n w_{ij} \\|\\mathbf{U}_{i\\cdot} - \\mathbf{U}_{j\\cdot}\\|_2 + \\gamma \\sum_{j = 1}^T \\omega_i \\|\\mathbf{U}_{\\cdot j}\\mathbf{\\Psi}\\|_2\\]\nIf \\(\\Psi\\) represents a full-rank wavelet basis, this reduces to the sparse convex clustering problem on the wavelet coefficients, for which we provide a new and efficient algorithm. We apply this approach to a cell-subtyping problem in which (noisy) NMR spectra are sampled from individual brain cells. Our approach is able to identify the different cell lines most accurately and to identify key NMR peaks identifying each cell type.\nConvex Wavelet Clustering of NMR SpectraCitation:\n@INPROCEEDINGS{Weylandt:2021-ConvexWaveletClustering,\n  AUTHOR=\"Michael Weylandt and Thomas M. Roddenberry and Genevera I. Allen\",\n  TITLE=\"Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering\",\n  DOI=\"10.1109/DSLW51110.2021.9523413\"\n  PAGES={1-8},\n  CROSSREF=\"DSLW:2021\"\n}\n\n@PROCEEDINGS{DSLW:2021,\n  BOOKTITLE=\"DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021\",\n  YEAR=2021,\n  LOCATION=\"Toronto, Canada\"\n  EDITOR=\"Stark Draper and Z. Jane Wang\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:55-06:00"
    },
    {
      "path": "publications/fair_pca.html",
      "title": "Debiasing Projections for Fair Principal Components Analysis",
      "description": "Submitted Conference Publication",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        }
      ],
      "contents": "\nAbstract: Machine learning models can often reflect or exacerbate societal biases present in training data. While most attention in the nascent field of algorithmic fairness has focused on bias mitigation for supervised learning, biases can also be a problem for unsupervised analyses. With Principal Components Analysis (PCA), for example, biased dimension reduction can affect downstream analysis as well as affect data exploration and modeling decisions. In this paper, we propose a novel framework for Fair PCA that finds debiasing projections of the data, or projections that remove the effect from any protected attribute. We show that our approach has many advantages over existing formulations of Fair PCA including a fast, closed form solution, generality and wide applicability to many protect attributes, and its ease of interpretation and usage like projections in classical PCA. We demonstrate the bias mitigating advantages of our Fair PCA method compared to existing proposals for Fair PCA on several synthetic and benchmark datasets.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:55-06:00"
    },
    {
      "path": "publications/hepatoscore14.html",
      "title": "HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk",
      "description": "*Hepatology* 73(6), pp. 2278-2292. 2021.",
      "author": [
        {
          "name": "J. S. Morris et al.",
          "url": {}
        }
      ],
      "contents": "\nAbstract: Background and Aims: Therapeutic, clinical trial entry and stratification decisions for hepatocellular carcinoma (HCC) are made based on prognostic assessments, using clinical staging systems based on small numbers of empirically selected variables that insufficiently account for differences in biological characteristics of individual patients’ disease.\nApproach and Results: We propose an approach for constructing risk scores from circulating biomarkers that produce a global biological characterization of individual patient’s disease. Plasma samples were collected prospectively from 767 patients with HCC and 200 controls, and 317 proteins were quantified in a Clinical Laboratory Improvement Amendments–certified biomarker testing laboratory. We constructed a circulating biomarker aberration score for each patient, a score between 0 and 1 that measures the degree of aberration of his or her biomarker panel relative to normal, which we call HepatoScore. We used log-rank tests to assess its ability to substratify patients within existing staging systems/prognostic factors. To enhance clinical application, we constructed a single-sample score, HepatoScore-14, which requires only a subset of 14 representative proteins encompassing the global biological effects. Patients with HCC were split into three distinct groups (low, medium, and high HepatoScore) with vastly different prognoses (medial overall survival 38.2/18.3/7.1 months; P < 0.0001). Furthermore, HepatoScore accurately substratified patients within levels of existing prognostic factors and staging systems (P < 0.0001 for nearly all), providing substantial and sometimes dramatic refinement of expected patient outcomes with strong therapeutic implications. These results were recapitulated by HepatoScore-14, rigorously validated in repeated training/test splits, concordant across Myriad RBM (Austin, TX) and enzyme-linked immunosorbent assay kits, and established as an independent prognostic factor.\nConclusions: HepatoScore-14 augments existing HCC staging systems, dramatically refining patient prognostic assessments and therapeutic decision making and enrollment in clinical trials. The underlying strategy provides a global biological characterization of disease, and can be applied broadly to other disease settings and biological media.\nPublisher DOI: 10.1002/hep.31555\nPubMed: 32931023\nCitation:\n@ARTICLE{Morris:2021,\n  AUTHOR=\"J.S. Morris and M.M. Hassan and Y.E. Zohner and Z. Wang and L. Xiao and A. Rashid and A. Haque and R. Abdel-Wahad and Y.A. Mohamed and K.L. Ballard and R.A. Wolff and B. George and L. Li and G.I. Allen and M. Weylandt and D. Li and W. Wang and K. Raghav and J. Yao and H.M. Amin and A.O. Kaseb\",\n  TITLE=\"HepatoScore14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk\",\n  JOURNAL=\"Hepatology\",\n  VOLUME=73,\n  NUMBER=6,\n  PAGES=\"2278--2292\",\n  YEAR=2021,\n  DOI=\"10.1002/hep.31555\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:55-06:00"
    },
    {
      "path": "publications/mrsfpca.html",
      "title": "Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques",
      "description": "*CAMSAP 2019: Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing*, pp.500-504. 2019.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: We consider the problem of estimating multiple principal components using the recently-proposed Sparse and Functional Principal Components Analysis (SFPCA) estimator. We first propose an extension of SFPCA which estimates several principal components simultaneously using manifold optimization techniques to enforce orthogonality constraints. While effective, this approach is computationally burdensome so we also consider iterative deflation approaches which take advantage of existing fast algorithms for rank-one SFPCA. We show that alternative deflation schemes can more efficiently extract signal from the data, in turn improving estimation of subsequent components. Finally, we compare the performance of our manifold optimization and deflation techniques in a scenario where orthogonality does not hold and find that they still lead to significantly improved performance.\nPublisher DOI: 10.1109/CAMSAP45676.2019.9022486\nWorking Copy: ArXiv 1907.12012\nSummary: This work extends my previous work on Sparse and Functional PCA to the case of estimating multiple principal components. While classical PCA gives nicely orthogonal and simultaneously estimable1 components, the situation for sparse PCA is less compelling. I translate the rank-1 SFPCA problem to a rank-\\(k\\) optimization problem over a pair of generalized Stiefel manifolds: \\[\\text{arg max}_{\\mathbf{U} \\in \\mathcal{V}_{n\\times k}^{\\mathbf{S}_u}, \\mathbf{V} \\in \\mathcal{V}_{p \\times k}^{\\mathbf{S}_v}} \\text{Tr}(\\mathbf{U}^T\\mathbf{X}\\mathbf{V}) - \\lambda_{\\mathbf{U}} P_{\\mathbf{U}}(\\mathbf{U}) - \\lambda_{\\mathbf{V}} P_{\\mathbf{V}}(\\mathbf{V})\\] To solve this problem, I take advantage recent non-smooth manifold-constrained optimization schemes2 and show that superior solutions can be obtained for reasonable sized-problems. (These methods require one or more matrix decompositions at each iteration, so they don’t scale to very large data.) While directly solving the rank-\\(k\\) problem is best, I also propose some improvements to the greedy rank-1 strategy that guarantee weaker forms of orthogonality. These improvements cannot impose true orthogonality, but they do guarantee that the estimated principal components are “residual-orthogonal” which ultimately improves statistical performance:\nMethod\nTwo-Way Orthogonality \\(u^TX_tv = 0\\)\nOne-Way Orthogonality \\(u^TX_t, X_tv = 0\\)\nSubsequent Orthogonality \\(u^TX_{t + s}, X_{t + s}v = 0\\) for all \\(s\\)\nRobust to Scale\nHotelling Deflation\n✓\nX\nX\nX\nProjection Deflation\n✓\n✓\nX\nX\nSchur Deflation\n✓\n✓\n✓\n✓\nInterestingly, all these forms of orthogonality tend to give improved performance, even when the underlying signal is not orthogonal.\nComparison of Multi-Rank SFPCA with Traditional SVD PCARelated Software: MoMA\nCitation:\n@INPROCEEDINGS{Weylandt:2019-MRSFPCA,\n  TITLE=\"Multi-Rank Sparse and Functional {PCA}: Manifold Optimization and Iterative Deflation Techniques\",\n  AUTHOR=\"Michael Weylandt\",\n  CROSSREF={CAMSAP:2019},\n  DOI=\"10.1109/CAMSAP45676.2019.9022486\",\n  PAGES={500-504}\n}\n\n@PROCEEDINGS{CAMSAP:2019,\n  TITLE=\"{CAMSAP} 2019: Proceedings of the 8\\textsuperscript{th} {IEEE} Workshop on Computational Advances in Multi-Sensor Adaptive Processing\",\n  YEAR=2019,\n  LOCATION=\"Le Gosier, Guadaloupe\",\n  EDITOR=\"Geert Leus and Antonio G. Marques\"\n}\nTaking the eigendecomposition as an “atomic” operation↩\nUnpublished as of 2019: now published as\n- S. Chen, S. Ma, A. M.-C. So, and T. Zhang, “Proximal gradient method for nonsmooth optimization over the Stiefel manifold,” SIAM Journal on Optimization, vol. 30, no. 1, pp. 210–239, 2020.\n- S. Chen, S. Ma, L. Xue, and H. Zou, “An alternating manifold proximal gradient method for sparse principal component analysis and sparse canonical correlation analysis,” INFORMS Journal on Optimization, vol. 2, no. 3, pp. 192– 208, 2020.\n\n↩\n",
      "last_modified": "2022-02-15T22:48:56-06:00"
    },
    {
      "path": "publications/network_convex_clustering.html",
      "title": "Network Clustering for Latent State and Changepoint Detection",
      "description": "Submitted Conference Publication",
      "author": [
        {
          "name": "Madeline Navarro",
          "url": {}
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        },
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Network models provide a powerful and flexible framework for analyzing a wide range of structured data sources. In many situations of interest, however, multiple networks can be constructed to capture different aspects of an underlying phenomenon or to capture changing behavior over time. In such settings, it is often useful to  together related networks in attempt to identify patterns of common structure. In this paper, we propose a convex approach for the task of network clustering. Our approach uses a convex fusion penalty to induce a smoothly-varying tree-like cluster structure, eliminating the need to select the number of clusters . We provide an efficient algorithm for convex network clustering and demonstrate its effectiveness on synthetic examples.\nWorking Copy: ArXiv 2111.01273\nSummary: We consider the problem of clustering a set of networks. We “stack” these networks into a tensor format and then re-rexpress the network clustering problem as a tensor co-clustering problem. We solve this tensor co-clustering problem using a variant of convex (co-)clustering and provide an efficient algorithm for the resulting optimization problem.\nSchematic Diagram of Convex Network Clustering\n\n\n",
      "last_modified": "2022-02-15T22:48:56-06:00"
    },
    {
      "path": "publications/ng_volatility.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:57-06:00"
    },
    {
      "path": "publications/sfpca.html",
      "title": "Sparse and Functional Principal Components Analysis",
      "description": "*DSW 2019: Proceedings of the IEEE Data Science Workshop 2019*, pp.11-16. 2019.",
      "author": [
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        },
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Regularized variants of Principal Components Analysis, especially Sparse PCA and Functional PCA, are among the most useful tools for the analysis of complex high-dimensional data. Many examples of massive data, have both sparse and functional (smooth) aspects and may benefit from a regularization scheme that can capture both forms of structure. For example, in neuro-imaging data, the brain’s response to a stimulus may be restricted to a discrete region of activation (spatial sparsity), while exhibiting a smooth response within that region. We propose a unified approach to regularized PCA which can induce both sparsity and smoothness in both the row and column principal components. Our framework generalizes much of the previous literature, with sparse, functional, two-way sparse, and two-way functional PCA all being special cases of our approach. Our method permits flexible combinations of sparsity and smoothness that lead to improvements in feature selection and signal recovery, as well as more interpretable PCA factors. We demonstrate the efficacy of our method on simulated data and a neuroimaging example on EEG data.\nPublisher DOI: 10.1109/DSW.2019.8755778\nWorking Copy: ArXiv 1309.2895\nSummary: We give a unified proposal for regularized PCA, allowing for both sparsity and smoothness (functional structure) in both the estimated observation and feature embeddings (left and right singular values): \\[\\text{arg max}_{\\mathbf{u} \\in \\overline{\\mathbb{B}}^n_{\\mathbf{S}_{\\mathbf{u}}}, \\mathbf{v} \\in \\overline{\\mathbb{B}}^p_{\\mathbf{S}_{\\mathbf{v}}}} \\mathbf{u}^T\\mathbf{X}\\mathbf{v} - \\lambda_{\\mathbf{u}} P_{\\mathbf{u}}(\\mathbf{u}) - \\lambda_{\\mathbf{v}} P_{\\mathbf{v}}(\\mathbf{v})\\] Our approach incorporates a variety of prior proposals as special cases, including:\nClassical (Unregularized) PCA\nOne-Way Sparse PCA: Shen and Huang (J. Multivariate Analysis, 2008)\nTwo-Way Sparse PCA: Allen, Grosenick, and Taylor (J. American Statistical Association, 2014) and Witten, Tibshirani, and Hastie (Biostatistics, 2009)\nOne-Way Functional PCA: Silverman (Annals of Statistics, 1996) and Huan, Shen, and Buja (Electronic Journal of Statistics, 2008)\nTwo-Way Functional PCA: Huang, Shen, and Buja (J. American Statistical Association, 2009)\nWe prove that our proposal is well-formulated, i.e., that all constraints are either binding or clearly slack with no masking, and give an efficient algorithm for finding a Nash point of the non-convex SFPCA problem. Finally, we apply our result in simulations and to EEG data and show that it finds meaningful and useful principal components.\nThe ArXiv-only appendices of this paper have quite a lot of useful content, including detailed analysis of the SFPCA problem and equivalencies to other regularized PCA schemes. Appendix C gives a thorough review of the regularized PCA literature as of 2019.\nIn Silico Comparison of SFPCA with other PCA VariantsRecovering space-time localized left and right singular vectors with SFPCARelated Software: MoMA\nCitation:\n@INPROCEEDINGS{Allen:2019,\n  TITLE=\"Sparse and Functional Principal Components Analysis\",\n  AUTHOR=\"Genevera I. Allen and Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755778\",\n  PAGES={11-16}\n}\n\n@PROCEEDINGS{DSW:2019,\n  TITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T22:48:58-06:00"
    },
    {
      "path": "publications/tree_reproduction.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:58-06:00"
    },
    {
      "path": "publications/trout.html",
      "title": "Automatic Registration and Convex Clustering of Time Series",
      "description": "*ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing*, pp.5609-5613. 2021",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "George Michailidis",
          "url": {}
        }
      ],
      "contents": "\nAbstract: Clustering of time series data exhibits a number of challenges not present in other settings, notably the problem of registration (alignment) of observed signals. Typical approaches include pre-registration to a user-specified template or time warping approaches which attempt to optimally align series with a minimum of distortion. For many signals obtained from recording or sensing devices, these methods may be unsuitable as a template signal is not available for pre-registration, while the distortion of warping approaches may obscure meaningful temporal information. We propose a new method for automatic time series alignment within a clustering problem. Our approach, Temporal Registration using Optimal Unitary Transformations (TROUT), is based on a novel dissimilarity measure between time series that is easy to compute and automatically identifies optimal alignment between pairs of time series. By embedding our new measure in a optimization formulation, we retain well-known advantages of computational and statistical performance. We provide an efficient algorithm for TROUT-based clustering and demonstrate its superior performance over a range of competitors.\nPublisher DOI: 10.1109/ICASSP39728.2021.9414417\nWorking Copy: ArXiv 2012.04756\nSummary: Clustering time series is a common problem in many applications, but it is much more difficult when analyzing non-aligned signals. Typically, signals are pre-registered to some template or common alignment, but this assumes that such a template exists, when discovering “standard signals” is often the goal of clustering in the first case. We instead propose to align signals to each other within the clustering problem. Given two signals, the optimal alignment between them can be found by an orthogonal rotation (phase-shift), with closed form: this motivates TROUT-clustering \\[\\text{argmin}_{\\mathbf{U} \\in \\mathbb{C}^{n \\times p}} \\frac{1}{2}d_{\\text{TROUT}}(\\mathbf{U}, \\mathbf{X})^2 + \\lambda \\sum_{\\substack{i, j = 1 \\\\ i < j}}^n w_{ij} \\|\\mathbf{U}_{i\\cdot} - \\mathbf{U}_{j\\cdot}\\|_q\\] where \\[d_{\\text{TROUT}}(\\mathbf{u}, \\mathbf{x}) = \\min_{\\theta \\in [0, 2\\pi)} \\|\\mathbf{u} - e^{i \\theta} \\mathbf{x}\\|_2 \\implies  d_{\\text{TROUT}}(\\mathbf{U}, \\mathbf{X})^2 = \\sum_{i=1}^n d_{\\text{TROUT}}(\\mathbf{U}_{i\\cdot}, \\mathbf{X}_{i\\cdot})^2.\\] is \\(d_{\\text{TROUT}}\\) our “phase-adaptive” (optimally aligning) distance1 To solve this problem, we develop a Majorization-Minimization scheme, the sub-problems of which have closed form solution, yielding an efficient algorithm.\nNote that the clustering formulation is designed so that each observation (row of \\(\\mathbf{X}\\)) is rotated to coincide with the shared centroids (row of \\(\\mathbf{U}\\)) rather than to each other: this avoids the registration problem entirely, but does give a solution that is only identified up to phase. Our method performs well under a variety of noise conditions and often even out-performs traditional clustering methods with oracle pre-registration.\nExamples of TROUT ClusteringComparison of TROUT, Time-Warping, and Traditional ClusteringCitation:\n@INPROCEEDINGS{Weylandt:2021-TROUT,\n  AUTHOR=\"Michael Weylandt and George Michailidis\",\n  TITLE=\"Automatic Registration and Convex Clustering of Time Series\",\n  DOI=\"10.1109/ICASSP39728.2021.9414417\"\n  PAGES={5609-5613},\n  CROSSREF=\"ICASSP:2021\"\n}\n\n@PROCEEDINGS{ICASSP:2021,\n  BOOKTITLE=\"ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing\",\n  YEAR=2021,\n  LOCATION=\"Toronto, Canada\"\n  EDITOR=\"Tim Davidson and Dong Yu\"\n}\n\n\\(d_{\\text{TROUT}}\\) does not define a true metric, so the TROUT-clustering problem is not guaranteed to be convex.↩\n",
      "last_modified": "2022-02-15T22:48:58-06:00"
    },
    {
      "path": "publications.html",
      "title": "Publications",
      "author": [],
      "contents": "\nBelow is a list of my publications: for more details on each publication, click for details.\nSubmitted\nM. Weylandt and G. Michailidis. “Coupled CP Decomposition for Principal Components Analysis of Symmetric Networks”\nM. Weylandt and G. I. Allen. “Debiasing Projections for Fair Principal Components Analysis”\nM. Navarro, G. I. Allen, and M. Weylandt. “Network Clustering for Latent State and Changepoint Detection”\nM. Weylandt, Y. Han, and K. B. Ensor. “Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating Futures Market Realized Volatility”\nWinner of the ASA Section on Business and Economic Statistics (B&E) 2020 Student Paper Competition\nA.P. Drager, M. Weylandt, G. Chuyong, D. Kenfack, D.W. Thomas, and A.E. Dunham. “Ecological correlates of reproductive status in a guild of Afrotropical understory trees.”\nPublished\nM. Weylandt, G. Michailidis, and T. M. Roddenberry. “Sparse Partial Least Squares for Coarse Noisy Graph Alignment.” SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop 2021, pp.561-565. 2021.\nM. Weylandt, T.M. Roddenberry, and G. I. Allen. “Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering.” DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021. pp.1-8. 2021.\nM. Weylandt and G. Michailidis. “Automatic Registration and Convex Clustering of Time Series.” ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, pp.5609-5613. 2021.\nJ.S. Morris, M.M. Hassan, Y.E. Zohner, Z. Wang, L. Xiao, A. Rashid, A. Haque, R. Abdel-Wahad, Y.A. Mohamed, K.L. Ballard, R.A. Wolff, B. George, L. Li, G. I. Allen, M. Weylandt, D. Li, W. Wang, K. Raghav, J. Yao, H.M. Amin, and A.O. Kaseb. “HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk.” Hepatology 73(6), pp. 2278-2292. 2021.\nM. Weylandt. “Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques.” CAMSAP 2019: Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pp.500-504. 2019.\nM. Weylandt, J. Nagorski, and G. I. Allen. “Interactive Visualizations and Fast Computation for Convex Clustering via Algorithmic Regularization.” Journal of Computational and Graphical Statistics 29(1), pp. 87-96. 2020.\nWinner of the ASA Section on Statistical Learning and Data Science (SLDS) 2019 Student Paper Competition\nM. Weylandt. “Splitting Methods for Convex Bi-Clustering and Co-Clustering.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.237-244. 2019.\nG. I. Allen and M. Weylandt. “Sparse and Functional Principal Components Analysis.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.11-16. 2019.\n\n\n\n",
      "last_modified": "2022-02-15T22:48:58-06:00"
    },
    {
      "path": "software.html",
      "title": "Michael Weylandt",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-02-15T22:48:59-06:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
