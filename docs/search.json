[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software Packages",
    "section": "",
    "text": "Software development is a key component of my research agenda. In developing software implementing statistical methodology, I aim to provide reasonable theoretically well-justified defaults and elegant APIs that encourage statistical best practices. Most of my work can be found on my personal GitHub page or under the DataSlingers organization. Key packages are listed below:"
  },
  {
    "objectID": "software.html#stable",
    "href": "software.html#stable",
    "title": "Software Packages",
    "section": "Stable",
    "text": "Stable\n\nClustRViz: Interactive Visualizations and Fast Computation for Convex Clustering.\nExclusiveLasso: Generalized Linear Models with the Exclusive Lasso Penalty\nxtsPlots: Additional Plotting Tools for xts Objects"
  },
  {
    "objectID": "software.html#under-development",
    "href": "software.html#under-development",
    "title": "Software Packages",
    "section": "Under Development",
    "text": "Under Development\n\nBayesHMM: Frequentist and Bayesian Inference for Hierarchical Hidden Markov Models Using Stan\nMoMA: Modern Multivariate Analysis - PCA, PLS, CCA, and LDA with Sparsity, Smoothness, and Structure\nsgdnet: Sparse Linear Models for Big Data via Stochastic Average Gradient"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Michael Weylandt is an Assistant Professor in the Paul H. Chook Department of Information Systems and Statistics at the Zicklin School of Business, Baruch College, CUNY. Before joining Baruch, he held postdoctoral positions at Sandia National Laboratories and through the US Intelligence Community Postdoctoral Fellowship. His work has been recognized with best paper awards from the American Statistical Association in both Statistical Learning and Data Science and in Business & Economic Statistics. He has served as a mentor in the Google Summer of Code program for 7 years on behalf of the R Foundation for Statistical Computing and previously held an NSF Graduate Research Fellowship. He received a Bachelor’s of Science in Engineering from Princeton University in 2008 and a Ph.D. in Statistics from Rice University in 2020, where he was supervised by Katherine B. Ensor.\nFor more details, see my Full CV."
  },
  {
    "objectID": "software/MoMA.html",
    "href": "software/MoMA.html",
    "title": "MoMA",
    "section": "",
    "text": "The MoMA R package implements the Sparse and Functional PCA (SFPCA) framework, as well as its extensions to CCA, PLS, and Linear Discriminant Analysis. In addition to standard sparse (Lasso) penalization, the package also allows for the group lasso, the fused lasso, convex clustering, SCAD, MCP, and SLOPE penalization of both the left and right singular vectors.\nThe core numerical routines of this package are stable, but the user interface and tuning parameter selection routines are still a work in progress. If you are interested in collaborating on further development of this package, please get in touch.\nDirect Link: http://github.com/DataSlingers/MoMA\nPackage Documentation: https://DataSlingers.github.io/MoMA/\nRelated Publications: Sparse and Functional PCA, Multi-Rank SFPCA, Coarse Noisy Graph Alignment"
  },
  {
    "objectID": "software/bayeshmm.html",
    "href": "software/bayeshmm.html",
    "title": "BayesHMM",
    "section": "",
    "text": "The BayesHMM R package implements a comprehensive framework for Bayesian inference of Hidden Markov Models, including a tailored DSL for model specification, automatic translation to the Stan modeling language, and a variety of diagnostic tools. Under the hood, the package uses the techniques we discuss in our tutorial, but additional performance improvements could be achieved by rewriting the backend to use Stan’s new built-in HMM tools. The package should be stable, but has not been thoroughly validated.\nDirect Link: https://github.com/luisdamiano/BayesHMM\nPackage Documentation: https://luisdamiano.github.io/BayesHMM/\nRelated Publications: A Tutorial on Hidden Markov Models using Stan"
  },
  {
    "objectID": "software/xtsplots.html",
    "href": "software/xtsplots.html",
    "title": "xtsPlots",
    "section": "",
    "text": "The xtsPlots R package implements an alternative framework for plotting time series in R. The package was originally developed in 2012 and pre-dates the rise of modern plotting toolkits in R. While the package is still usable, it is no longer actively developed and the ideas in it have been superseded by a newer plotting engine in the xts package.\nDirect Link: https://github.com/michaelweylandt/xtsPlots"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hello and welcome to my website! My name is Michael Weylandt.\nI am a (tenure-track) Assistant Professor in the Paul H. Chook Department of Information Systems and Statistics in the Zicklin School of Business at Baruch College, City University of New York (CUNY). My research is in statistical machine learning, with particular recent interests in climate statistics, network data, and machine learning fairness. Before joining Baruch, I was a Postdoctoral Appointee in the Statistical Sciences group at Sandia National Laboratories, working with J. Derek Tucker and Laura P. Swiler, and before that I was an US Intelligence Community Postdoctoral Research Fellow hosted at the University of Florida Informatics Institute, where I was advised by George Michailidis as well as two mentors from the Intelligence Community. I earned my Ph.D. in Statistics at Rice University in 2020 under the supervision of Kathy Ensor, worked in finance from 2013 to 2015, and taught at Sherborne School in the UK from 2012 to 2013.\nFor a more formal bio, please see my CV and my About page. More detail on my Publications and Software can be found on those pages."
  },
  {
    "objectID": "publications/convex_wavelet_clustering.html",
    "href": "publications/convex_wavelet_clustering.html",
    "title": "Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering",
    "section": "",
    "text": "Abstract: Clustering is a ubiquitous problem in data science and signal processing. In many applications where we observe noisy signals, it is common practice to first denoise the data, perhaps using wavelet denoising, and then to apply a clustering algorithm. In this paper, we develop a sparse convex wavelet clustering approach that simultaneously denoises and discovers groups. Our approach utilizes convex fusion penalties to achieve agglomeration and group-sparse penalties to denoise through sparsity in the wavelet domain. In contrast to common practice which denoises then clusters, our method is a unified, convex approach that performs both simultaneously. Our method yields denoised (wavelet-sparse) cluster centroids that both improve interpretability and data compression. We demonstrate our method on synthetic examples and in an application to NMR spectroscopy.\nPublisher DOI: 10.1109/DSLW51110.2021.9523413\nWorking Copy: ArXiv 2012.04762\n\nSummary: We consider the problem of clustering highly noisy signals. The dual nature of this problem gives rise to a puzzle: if we denoise the individual signals and then cluster, the resulting cluster centroids will be noisy, but if we cluster and then denoise, our clustering accuracy will be sub-par. We instead propose to simultaneous denoise and to cluster by combining wavelet soft-thresholding with convex clustering. The resulting approach identifies clusters which are denoised (wavelet sparse), while retaining the excellent statistical performance of convex clustering:\n Our approach be expressed as the following convex optimization problem:\n[ | - |F^2 + {}^n w_{ij} |{i} - {j}|2 + {j = 1}^T i |{j}|_2]\nIf \\(\\Psi\\) represents a full-rank wavelet basis, this reduces to the sparse convex clustering problem on the wavelet coefficients, for which we provide a new and efficient algorithm. We apply this approach to a cell-subtyping problem in which (noisy) NMR spectra are sampled from individual brain cells. Our approach is able to identify the different cell lines most accurately and to identify key NMR peaks identifying each cell type.\n\n\n\nConvex Wavelet Clustering of NMR Spectra\n\n\n\nCitation:\n@INPROCEEDINGS{Weylandt:2021-ConvexWaveletClustering,\n  AUTHOR=\"Michael Weylandt and T. Mitchell Roddenberry and Genevera I. Allen\",\n  TITLE=\"Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering\",\n  DOI=\"10.1109/DSLW51110.2021.9523413\"\n  PAGES={1-8},\n  CROSSREF=\"DSLW:2021\"\n}\n\n@PROCEEDINGS{DSLW:2021,\n  BOOKTITLE=\"DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021\",\n  YEAR=2021,\n  LOCATION=\"Toronto, Canada\"\n  EDITOR=\"Stark Draper and Z. Jane Wang\"\n}"
  },
  {
    "objectID": "publications/mrsfpca.html",
    "href": "publications/mrsfpca.html",
    "title": "Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques",
    "section": "",
    "text": "Abstract: We consider the problem of estimating multiple principal components using the recently-proposed Sparse and Functional Principal Components Analysis (SFPCA) estimator. We first propose an extension of SFPCA which estimates several principal components simultaneously using manifold optimization techniques to enforce orthogonality constraints. While effective, this approach is computationally burdensome so we also consider iterative deflation approaches which take advantage of existing fast algorithms for rank-one SFPCA. We show that alternative deflation schemes can more efficiently extract signal from the data, in turn improving estimation of subsequent components. Finally, we compare the performance of our manifold optimization and deflation techniques in a scenario where orthogonality does not hold and find that they still lead to significantly improved performance.\nPublisher DOI: 10.1109/CAMSAP45676.2019.9022486\nWorking Copy: ArXiv 1907.12012\nSummary: This work extends my previous work on Sparse and Functional PCA to the case of estimating multiple principal components. While classical PCA gives nicely orthogonal and simultaneously estimable1 components, the situation for sparse PCA is less compelling. I translate the rank-1 SFPCA problem to a rank-\\(k\\) optimization problem over a pair of generalized Stiefel manifolds: [{ {nk}^{u}, {p k}^{v}} (^T) - {} P_{}() - {} P{}()] To solve this problem, I take advantage recent non-smooth manifold-constrained optimization schemes2 and show that superior solutions can be obtained for reasonable sized-problems. (These methods require one or more matrix decompositions at each iteration, so they don’t scale to very large data.) While directly solving the rank-\\(k\\) problem is best, I also propose some improvements to the greedy rank-1 strategy that guarantee weaker forms of orthogonality. These improvements cannot impose true orthogonality, but they do guarantee that the estimated principal components are “residual-orthogonal” which ultimately improves statistical performance:\nInterestingly, all these forms of orthogonality tend to give improved performance, even when the underlying signal is not orthogonal.\nRelated Software: MoMA\nCitation:"
  },
  {
    "objectID": "publications/mrsfpca.html#footnotes",
    "href": "publications/mrsfpca.html#footnotes",
    "title": "Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTaking the eigendecomposition as an “atomic” operation↩︎\nUnpublished as of 2019: now published as\n\n- S. Chen, S. Ma, A. M.-C. So, and T. Zhang, “Proximal gradient method for nonsmooth optimization over the Stiefel manifold,” SIAM Journal on Optimization, vol. 30, no. 1, pp. 210–239, 2020.\n\n- S. Chen, S. Ma, L. Xue, and H. Zou, “An alternating manifold proximal gradient method for sparse principal component analysis and sparse canonical correlation analysis,” INFORMS Journal on Optimization, vol. 2, no. 3, pp. 192– 208, 2020.↩︎"
  },
  {
    "objectID": "publications/ddcn.html",
    "href": "publications/ddcn.html",
    "title": "Optimal accuracy for linear sets of equations with the graph Laplacian",
    "section": "",
    "text": "Abstract: We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph—those that achieve optimality asymptotically with the graph size and those that do not—determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments.\nWorking Copy: ArXiv 2405.07877\n\nSummary: In this work, we consider linear systems of the form \\(Lx = b\\), where \\(L\\) is a suitable graph Laplacian. Classical analysis would suggest that these problems are very poorly conditioned, due to the many small near-zero singular values of \\(L\\). In practice, however, these systems are solved to high accuracy quite regularly.\nWe reconcile these observations through the notion of a data-dependent condition number: the classical condition number of a matrix is more formally its condition number with respect to inversion. And inverting a matrix directly is a terrible way to solve \\(Lx = b\\)! More formally, if \\(L\\) is well-conditioned for inversion, than \\(Lx = b\\) should be well-conditioned for any \\(b\\) - but that’s far stronger than what we need in practice, where we only consider a small range of \\(b\\). By instead performing a conditioning analysis of the specific \\(Lx = b\\) system, not just \\(L\\), we find that for many interesting \\(b\\), the problem is very well conditioned, with condition number approaching \\(1\\) for the linear system used to compute expected hitting times of simple random walks on very large undirected graphs.\nOur analysis is “classical” relying only on a suitable eigendecomposition of \\(L\\) and careful consideration of the problem geometry. A particularly useful result is Proposition 2, which gives bounds on the conditioning that can be computed without requiring any iterative solver to be used. (Classical conditioning analysis requires computing the spectrum of \\(L\\); general data-dependent conditioning analysis requires knowledge of the true solution \\(x\\) and the eigendecomposition of \\(L\\)) This sort of a priori bound is immensely useful in practice, as it allows us to decide whether a PageRank problem or its ‘dual’ hitting time problem will yield a better conditioned system for a particular analysis.\n\n\n\nPractical Impact of Data-Dependent Condition Numbers for Graph Centrality Problems\n\n\nThe presentation in this paper is quite brief and intentionally self-contained. A longer manuscript, showing how our results can be applied to discrete potential problems and Laplacian systems generally, is in preparation.\n\nCitation:\n@ARTICLE{Lehoucq:2024\n  AUTHOR=\"Richard B. Lehoucq and Michael Weylandt and Jonathan W. Berry\",\n  TITLE=\"Optimal accuracy for linear sets of equations with the graph {L}aplacian\",\n  JOURNAL=\"ArXiv Pre-Print 2405.07877\",\n  YEAR=2024,\n  URL=\"https://arxiv.org/abs/2405.07877\"\n}"
  },
  {
    "objectID": "publications/coclustering_algs.html",
    "href": "publications/coclustering_algs.html",
    "title": "Splitting Methods for Convex Bi-Clustering and Co-Clustering",
    "section": "",
    "text": "Abstract: Co-Clustering, the problem of simultaneously identifying clusters across multiple aspects of a data set, is a natural generalization of clustering to higher-order structured data. Recent convex formulations of bi-clustering and tensor co-clustering, which shrink estimated centroids together using a convex fusion penalty, allow for global optimality guarantees and precise theoretical analysis, but their computational properties have been less well studied. In this note, we present three efficient operator-splitting methods for the convex co-clustering problem: a standard two-block ADMM, a Generalized ADMM which avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests, and experimental evidence confirms, that the Generalized ADMM is far more efficient for large problems.\nPublisher DOI: 10.1109/DSW.2019.8755599\nWorking Copy: ArXiv 1901.06075\n\nSummary: Chi and Lange (J. Computational and Graphical Statistics, 2015) established that splitting methods are an easy-to-implement highly-performant way to solve convex clustering problems. In this work, I extend their analysis to the general case of tensor co-clustering, considering three operator-splitting schemes:\n\nThe classic ADMM\nA generalized (quadratically perturbed) ADMM\nDavis-Yin splitting (equivalent to AMA for this problem)\n\nClassical ADMM has the best per iteration performance, but requires solving a (tensor) Sylvester equation at each iteration, which is typically prohibitive. To avoid this, I construct a generalized ADMM scheme that avoids any matrix decompositions or inverses in the updates.\n\\[\\begin{align*}\n  \\mathbf{U}^{(k+1)} &= \\left(\\alpha \\mathbf{U}^{(k)} + \\mathbf{X} + \\rho\\mathbf{D}_{\\text{row}}^T(\\mathbf{V}^{(k)} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}} - \\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k)}) \\right. \\\\ &\\left.\\qquad+ \\rho(\\mathbf{V}^{(k)}_{\\text{col}} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}} - \\mathbf{U}^{(k)}\\mathbf{D}_{\\text{col}})\\mathbf{D}_{\\text{col}}^T \\right)/(1+\\alpha) \\\\\n\\begin{pmatrix} \\mathbf{V}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{V}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{row}, q}}(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}}) \\\\\n  \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{col}, q}}(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}})  \\end{pmatrix} \\\\\n    \\begin{pmatrix} \\mathbf{Z}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{Z}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\mathbf{Z}^{(k)}_{\\text{row}} + \\rho(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} - \\mathbf{V}^{(k+1)}_{\\text{row}}) \\\\ \\mathbf{Z}^{(k)}_{\\text{col}} + \\rho(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} - \\mathbf{V}^{(k+1)}_{\\text{col}})\n\\end{pmatrix}\n\\end{align*}\\] This method has slightly worse per iteration performance, but has superior wall clock performance and better computational complexity.\n\n\n\nPer Iteration Performance\n\n\n\n\n\nWall Clock Performance\n\n\nThe generalized ADMM is used internally in my clustRviz software for both the exact solver and the CBASS pathwise algorithm.\nRelated Software: clustRviz\n\nCitation:\n@INPROCEEDINGS{Weylandt:2019-BiClustering,\n  TITLE=\"Splitting Methods for Convex Bi-Clustering and Co-Clustering\",\n  AUTHOR=\"Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755599\",\n  PAGES={237-244}\n}\n\n@PROCEEDINGS{DSW:2019,\n  BOOKTITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}"
  },
  {
    "objectID": "publications/tree_reproduction.html",
    "href": "publications/tree_reproduction.html",
    "title": "Ecological correlates of reproductive status in a guild of Afrotropical understory trees",
    "section": "",
    "text": "Abstract: The relative abundance patterns of tropical trees have been of interest since the expeditions of Alfred Russel Wallace, but little is known about how differences in relative abundance relate to reproductive patterns. Flowering is resource-dependent and fitness differences as well as differences in the quality of the abiotic and biotic neighborhood may contribute to the variation in reproductive status responsible for population-level flowering patterns. This variation determines the density and distance between flowering conspecifics and may alter relative abundance extremes among species during reproduction, factors known to influence pollination success. We collected flowering status data for a guild of twenty-three co-occurring tree species that flower in the understory of the Korup Forest Dynamics Plot in Cameroon. We examined how the occurrence and location of reproductive events were related to spatial patterns of adult abundance, focal tree size, neighborhood crowding, and habitat, while accounting for the influence of shared ancestry. Across species, the probability of flowering was higher for individuals of rarer species and for larger individuals but was unrelated to neighborhood crowding or habitat differences. Relative abundance extremes were reduced when only flowering individuals were considered, leading to a negative relationship between plot abundance and flowering probability at the species level that was not structured by shared ancestry. Spatially, flowering conspecifics tended to be overdispersed relative to all adult conspecifics. Rare species are predicted to suffer Allee effects or reduced fitness due to the difficulty of finding mates at low densities and frequencies. Here, however, rare species appear to maximize the size of their mate pool, compared to abundant species. If this partial “leveling of the playing field” during reproduction is typical, it has consequences for our understanding of biodiversity maintenance and species coexistence in tropical forests.\nWorking Paper: BioRXiv 10.1101/2021.01.14.426416"
  },
  {
    "objectID": "publications/fairstacks.html",
    "href": "publications/fairstacks.html",
    "title": "To the Fairness Frontier and Beyond: Identifying, Quantifying, and Optimizing the Fairness-Accuracy Pareto Frontier",
    "section": "",
    "text": "Abstract: Algorithmic fairness has emerged as an important consideration when developing and deploying machine learning models to make high-stakes societal decisions. Yet, improved fairness often comes at the expense of model accuracy. While aspects of the fairness-accuracy tradeoff have been studied, most work reports the fairness and accuracy of various models separately; this makes model comparisons nearly impossible without a unified model-agnostic metric that reflects the Pareto optimal balance of the two desiderata. In this paper, we seek to identify, quantify, and optimize the empirical Pareto frontier of the fairness-accuracy tradeoff, defined as the highest attained accuracy at every level of fairness for a collection of fitted models. Specifically, we identify and outline the empirical Pareto frontier through our Tradeoff-between-Fairness-and-Accuracy (TAF) Curves; we then develop a single metric to quantify this Pareto frontier through the weighted area under the TAF Curve which we term the Fairness-Area-Under-the-Curve (FAUC). Our TAF Curves provide the first empirical, model-agnostic characterization of the Pareto frontier, while our FAUC provides the first unified metric to impartially compare model families in terms of both fairness and accuracy. Both TAF Curves and FAUC are general and can be employed with all group fairness definitions and accuracy measures. Next, we ask: Is it possible to expand the empirical Pareto frontier and thus improve the FAUC for a given collection of fitted models? We answer in the affirmative by developing a novel fair model stacking framework, FairStacks. FairStacks solves a convex program to maximize the accuracy of a linear combination of fitted models subject to a constraint on score-based model bias. We show that optimizing with FairStacks always expands the empirical Pareto frontier and improves the FAUC; we additionally study other theoretical properties of our proposed approach. Finally, we empirically validate TAF, FAUC, and FairStacks through studies on several real benchmark data sets, showing that FairStacks leads to major improvements in FAUC that outperform existing algorithmic fairness approaches.\nWorking Copy: ArXiv 2206.00074\n†These authors contributed equally.\n\nSummary: Given recent interest in questions of ML/Algorithmic fairness, we pause to ask how best to compare fair ML models. Specifically, while it is easy to compare models on fairness or accuracy, it is less obvious how to compare them on both axes. The difficulty of this question is compounded by the fact that many of these models have a “trade-off” parameter that allows the analyst to balance fairness and (overall) accuracy in a problem-specific way. We take inspiration from the economic / multi-objective optimization literature and consider the Pareto frontier attainable by a model (possibly as the tradeoff parameter is varied). We give an efficient algorithm for finding this Pareto frontier (TAF) and show that an AUC-type measure (FAUC) can be used to compare fair ML models under any utility function. A significant advantage of our framework is that it is model- and definition-agnostic, so it is possible to give an unbiased comparison of models which are trained under different loss functions or constraints. Having developed the TAF/FAUC framework, we next apply it to a fair model stacking framework (FairStacks) and show that ensembling can always improve the fairness/accuracy tradeoff of any set of models. The stacking context is particularly useful for fair ML as it allows the analyst to use powerful and potentially pre-trained base learners while still achieving fairness goals.\n\n\n\nComparison of Fair ML Techniques Using TAF/FAUC Framework\n\n\nWhile the main contributions of the paper are the preference-theoretic analysis of the model evaluation question and the FairStacks meta-learner, I think this paper has several other useful theoretical contributions which are worth future study:\n\nWe give sufficient conditions under which convex (score-based) fairness constraints translate to decision fairness, which is typically what is actually used in practice.\nWe connect fairness and model combinations with randomization and convex combinations of statistical testing procedures. These latter literatures are rich and give a straight-forward (and in many circumstances nearly optimal) way of “fair-ing” an existing base learner.\nWe give finite-sample distribution free guarantees on the out-of-sample accuracy and fairness of our method. These results are pretty standard applications of VC/PAC theory, but we expect they may be useful in audit studies of fairness as well.\n\n\n\n\nSummary of TAF/FAUC Framework and FairStacks Meta-Learner\n\n\nFinally, while our focus was on model evaluation and comparison, we also note that our framework can be used for data comparison, e.g., to compare the fairness of data collected before and after some real-world policy intervention.\n\nCitation:\n@ARTICLE{Little:2022,\n  AUTHOR=\"Camille Olivia Little and Michael Weylandt and Genevera I. Allen\",\n  TITLE=\"To the Fairness Frontier and Beyond: Identifying, Quantifying, and Optimizing the Fairness-Accuracy Pareto Frontier\",\n  YEAR=2022,\n  JOURNAL=\"ArXiv Pre-Print 2206.00074\"\n  DOI=\"10.48550/arXiv.2206.00074\"\n}"
  },
  {
    "objectID": "publications/sstpca.html",
    "href": "publications/sstpca.html",
    "title": "Multivariate Analysis for Multiple Network Data via Semi-Symmetric Tensor PCA",
    "section": "",
    "text": "Abstract: Network data are commonly collected in a variety of applications, representing either directly measured or statistically inferred connections between features of interest. In an increasing number of domains, these networks are collected over time, such as interactions between users of a social media platform on different days, or across multiple subjects, such as in multi-subject studies of brain connectivity. When analyzing multiple large networks, dimensionality reduction techniques are often used to embed networks in a more tractable low-dimensional space. To this end, we develop a framework for principal components analysis (PCA) on collections of networks via a specialized tensor decomposition we term Semi-Symmetric Tensor PCA or SS-TPCA. We derive computationally efficient algorithms for computing our proposed SS-TPCA decomposition and establish statistical efficiency of our approach under a standard low-rank signal plus noise model. Remarkably, we show that SS-TPCA achieves the same estimation accuracy as classical matrix PCA, with error proportional to the square root of the number of vertices in the network and not the number of edges as might be expected. Our framework inherits many of the strengths of classical PCA and is suitable for a wide range of unsupervised learning tasks, including identifying principal networks, isolating meaningful changepoints or outlying observations, and for characterizing the “variability network” of the most varying edges. Finally, we demonstrate the effectiveness of our proposal on simulated data and on an example from empirical legal studies. The techniques used to establish our main consistency results are surprisingly straightforward and may find use in a variety of other network analysis problems.\nWorking Copy: ArXiv 2202.04719\n\nSummary: We develop and approach for performing Principal Components Analysis (PCA) on network series data - sets of networks observed on the same node set. Network series data are observed when a changing network is observed over time (e.g., a social media network at the end of each week) or when a statistical network is estimated from time series data (e.g., stock market correlation networks in different years). We approach this problem by embedding the network series in a semi-symmetric tensor and performing tensor PCA on this representation.\n\n\n\nSchematic Diagram of Semi-Symmetric Tensor PCA\n\n\nWe rigorously analyze Tensor PCA in the semi-symmetric context, proving consistency under an analogue of the “low-rank + noise” model for matrix PCA: somewhat remarkably, despite the difficulty of the tensor setting, our results are within a logarithmic factor of classical PCA results. Our proof technique depends on the classical Davis-Kahan theorem (and some painful algebra) and we hope to apply it further to a variety of tensor decomposition problems. Finally, we apply our method to a variety of synthetic and real data sets and find some counter-intuitive results about the Supreme Court of the United States.\n\n\n\nTensor PCA applied to SCOTUS Voting Behaviors\n\n\nAn interesting quick of our analysis is that it allows for differing and possibly adversarial noise at each iteration of the tensor power method. While we don’t explore this fully, this has clear implications for both secure and online variants of our approach.\nPresentations: I presented this work at the 2022 Conference on New Advances in Statistics and Data Science (NASDS) and won the 2022 NASDS Best Poster Award. That poster can be found here. Slides from a longer (35-minute) presentation focusing on this work can be found here."
  },
  {
    "objectID": "publications/clustRviz.html",
    "href": "publications/clustRviz.html",
    "title": "Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization",
    "section": "",
    "text": "Winner of the ASA Section on Statistical Learning and Data Science (SLDS) 2019 Student Paper Competition\nAbstract: Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at https://github.com/DataSlingers/clustRviz.\nPublisher DOI: 10.1080/10618600.2019.1629943\nPubMed: 32982130\nWorking Copy: ArXiv 1901.01477\n\nSummary: In this paper, we look at efficient algorithms for the convex clustering problem: [{} = { ^{n p}} | - |F^2 + ({1 i &lt; j n} w_{ij} |{i} - {j}|_q)] Chi and Lange (JCGS, 2015) showed that the ADMM can solve this problem efficiently for fixed \\(\\lambda\\), but how should we think about this problem for multiple values of \\(\\lambda\\)? This is a more acute problem than in, e.g., sparse regression, because we need a large set of \\(\\lambda\\) values to construct a dendrogram representation of our data and the correct values of \\(\\lambda\\) are not known a priori. To make convex clustering practical, we need an algorithm to solve for a very fine grid of \\(\\lambda\\) values quickly: thankfully, we do not actually need highly accurate solutions as we are more interested in which points get clustered than in the (biased) cluster centroid estimates.\n\n\n\nConvex Clustering Solution Paths\n\n\nTo solve this problem, we propose a new form of extreme warm-start early-stopping that we call algorithmic regularization. By only increasing \\(\\lambda\\) by a small amount (to obtain a fine grid), warm-start techniques are sufficient to ensure that a single ADMM step gets a “good enough” solution. We term this approach CARP - Clustering via Algorithmic Regularization Paths\n\nInput:\n\nData matrix: \\(X \\in \\mathbb{R}^{n \\times p}\\)\nWeighted edge set: \\(\\mathcal{E} = \\{(e_l, w_l)\\}\\)\nRelaxation parameter: \\(\\rho \\in \\mathbb{R}_{&gt; 0}\\)\n\nPrecompute:\n\nDifference matrix \\(D \\in \\mathbb{R}^{|\\mathcal{E}| \\times n}\\)\nCholesky factor \\(L = \\textsf{chol}(I + \\rho D^TD) \\in \\mathbb{R}^{n \\times n}\\)\n\nInitialize:\n\n\\(U^{(0)} = X\\), \\(V^{(0)} = DX\\), \\(Z^{(0)} = V^{(0)}\\), \\(\\gamma^{(1)} = \\epsilon\\), \\(k = 1\\)\n\nRepeat until \\(\\|V^{(k - 1)}\\| = 0\\):\n\n\\(U^{(k)} = L^{-T}L^{-1}\\left[X + \\rho D^T(V^{(k - 1)} - Z^{(k - 1)})\\right]\\)\nIf \\(q = 1\\), for all \\((i, j)\\): [V_{ij}^{(k)} = {w_i ^{(k)}/ }((DU^{(k)} + Z^{(k - 1)}){ij})]\nIf \\(q = 2\\), for all \\(l\\): [V{(k)}{l} = (1 - )+(DU{(k)} + Z^{(k - 1)})_{l}]\n\\(Z^{(k)} = Z^{(k - 1)} + DU^{(k)} - V^{(k)}\\)\n\\(\\gamma^{(k + 1)} = t \\gamma^{(k)}\\)\n\\(k := k + 1\\)\n\nReturn \\(\\{(U^{(l)}, V^{(l)}\\}_{l = 0}^{k - 1}\\)\n\nWe rigorously justify our approach by showing convergence of the CARP path and the true solution set in the Hausdorff metric: \\[\\begin{align*}\nd_H(\\{\\mathbf{U}^{(k)}\\}, \\{\\hat{\\mathbf{U}}_{\\lambda}\\}) \\equiv \\max\\left\\{\\sup_{\\lambda} \\inf_k \\left\\|\\mathbf{U}^{(k)} - \\hat{\\mathbf{U}}_{\\lambda}\\right\\|, \\sup_{k} \\inf_\\lambda \\left\\|\\mathbf{U}^{(k)} - \\hat{\\mathbf{U}}_{\\lambda}\\right\\|\\right\\}  \\xrightarrow{(t, \\epsilon) \\to (1, 0)} 0\\\\\nd_H(\\{\\mathbf{Z}^{(k)}\\}, \\{\\hat{\\mathbf{Z}}_{\\lambda}\\}) \\equiv \\max\\left\\{\\sup_{\\lambda} \\inf_k \\left\\|\\mathbf{Z}^{(k)} - \\hat{\\mathbf{Z}}_{\\lambda}\\right\\|, \\sup_{k} \\inf_\\lambda \\left\\|\\mathbf{Z}^{(k)} - \\hat{\\mathbf{Z}}_{\\lambda}\\right\\|\\right\\} \\xrightarrow{(t, \\epsilon) \\to (1, 0)} 0\n\\end{align*}\\] This can be summarized as CARP finding “the whole path and nothing but the path.”\n\n\n\nHausdorff Convergence of CARP Algorithm\n\n\nExperimentally, we find that only moderately small step sizes are necessary to accurately estimate the dendrogram structure of the data. We extend this approach to Convex Bi-Clustering as well, giving the CBASS algorithm, which is a tortured acronym for Convex Bi-Clustering via Algorithmic Regularization with Small Steps. The version of CBASS in the paper does not guarantee Hausdorff convergence, but the modified version currently in the clustRviz R package does.\nOur theoretical analysis uses simple ingredients to show Hausdorff convergence:\n\nGeometric convergence of the ADMM (here implied by strong convexity, but this can often be relaxed)\nLipschitz continuity of the solution path\nA compact set of non-trivial \\(\\lambda\\) values\n\nCombining these in a “daisy-chaining” analysis (and several pages of inequalities) gives Hausdorff convergence. We believe this is a viable strategy for a wide range of statistical learning problems for which the “shape” of the solutions is more interesting than obtaining a single high-precision estimate.\nWe implement these algorithms in our clustRviz R package which is the fastest “whole path” solver available for the convex clustering. Later work has proposed better base algorithms than the ADMM and it would be interesting to use the Algorithmic Regularization strategy to boost these into “whole path” solvers.\n\n\n\nTiming Comparisons\n\n\nPresentations: I was invited to give a talk on my convex clustering work at the Lund University Statistical Learning Seminar, which can be viewed below:\n\n\nDirect YouTube Link. Slides from a more focused talk on the CARP and CBASS algorithms can be found here.\nRelated Software: The clustering methodology from this paper is implemented in my R package clustRviz. The algorithmic regularization scheme from this paper was applied to the Generalized ADMM from my paper on co-clustering algorithms to give the improved CBASS algorithm in the package.\n\nCitation:\n@ARTICLE{Weylandt:2020,\n  AUTHOR=\"Michael Weylandt and John Nagorski and Genevera I. Allen\",\n  TITLE=\"Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization\",\n  JOURNAL=\"Journal of Computational and Graphical Statistics\",\n  YEAR=2020,\n  VOLUME=29,\n  NUMBER=1,\n  PAGES={87-96},\n  DOI=\"10.1080/10618600.2019.1629943\",\n}"
  },
  {
    "objectID": "publications/wentland_attribution.html",
    "href": "publications/wentland_attribution.html",
    "title": "Conditional multi-step attribution for climate forcings",
    "section": "",
    "text": "Abstract: Attribution of climate impacts to a source forcing is critical to understanding, communicating, and addressing the effects of human influence on the climate. While standard attribution methods, such as optimal fingerprinting, have been successfully applied to long-term, widespread effects such as global surface temperature warming, they often struggle in low signal-to-noise regimes, typical of short-term climate forcings or climate variables which are loosely related to the forcing. Single-step approaches, which directly relate a source forcing and final impact, are unable to utilize additional climate information to improve attribution certainty. To address this shortcoming, this paper presents a novel multi-step attribution approach which is capable of analyzing multiple variables conditionally. A connected series of climate effects are treated as dependent, and relationships found in intermediary steps of a causal pathway are leveraged to better characterize the forcing impact. This enables attribution of the forcing level responsible for the observed impacts, while equivalent single-step approaches fail. Utilizing a scalar feature describing the forcing impact, simple forcing response models, and a conditional Bayesian formulation, this method can incorporate several causal pathways to identify the correct forcing magnitude. As an exemplar of a short-term, high-variance forcing, we demonstrate this method for the 1991 eruption of Mt. Pinatubo. Results indicate that including stratospheric and surface temperature and radiative flux measurements increases attribution certainty compared to analyses derived solely from temperature measurements. This framework has potential to improve climate attribution assessments for both geoengineering projects and long-term climate change, for which standard attribution methods may fail.\nWorking Copy: ArXiv 2409.01396\n\nCitation:\n@ARTICLE{Wentland:2024,\n  AUTHOR=\"Christopher R. Wentland and Michael Weylandt and Laura P. Swiler and Thomas S. Ehrmann and Diana Bull\",\n  TITLE=\"Conditional multi-step attribution for climate forcings\",\n  YEAR=2024,\n  DOI={10.48550/arXiv.2409.01396},\n  JOURNAL=\"ArXiv Pre-Print 2409.01396\"\n}"
  },
  {
    "objectID": "publications/network_convex_clustering.html",
    "href": "publications/network_convex_clustering.html",
    "title": "Network Clustering for Latent State and Changepoint Detection",
    "section": "",
    "text": "Abstract: Network models provide a powerful and flexible framework for analyzing a wide range of structured data sources. In many situations of interest, however, multiple networks can be constructed to capture different aspects of an underlying phenomenon or to capture changing behavior over time. In such settings, it is often useful to together related networks in attempt to identify patterns of common structure. In this paper, we propose a convex approach for the task of network clustering. Our approach uses a convex fusion penalty to induce a smoothly-varying tree-like cluster structure, eliminating the need to select the number of clusters . We provide an efficient algorithm for convex network clustering and demonstrate its effectiveness on synthetic examples.\nWorking Copy: ArXiv 2111.01273\n\nSummary: We consider the problem of clustering a set of networks. We “stack” these networks into a tensor format and then re-rexpress the network clustering problem as a tensor co-clustering problem. We solve this tensor co-clustering problem using a variant of convex (co-)clustering and provide an efficient algorithm for the resulting optimization problem.\n\n\n\nSchematic Diagram of Convex Network Clustering"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Interests",
    "section": "",
    "text": "My research is generally in the area of statistical machine learning. I interpret statistical in two senses: i) using tools from statistical theory to understand and improve methods, even those not originally developed from a statistical point-of-view; and ii) focused on the generation of insights and knowledge rather than just “black-box” predictive or generative power.1 Under that broad umbrella, I am particularly interested in structured dimension reduction, network analysis, and ML fairness, as well as various computational questions arising from methods work. My current application areas of interest include detection and attribution in climate analytics and the development of graph-inspired information retrieval systems.\nModern scientific experiments produce vast quantities of data that are nearly-impossible to analyze with traditional EDA (Exploratory Data Analysis) techniques. Moreover, these vast data sets typically have complex dependence structures that must be reflected in unsupervised analysis. I have developed techniques for flexibly incorporating structure into dimension reduction algorithms, most notably sparsity and smoothness and low-rank network structure. This line of work is particularly inspired by the analysis of large-scale climate simulations from global climate models. These models produce massive output, with significant temporal and spatial correlation. With collaborators from Sandia National Laboratories, I have applied regularized PCA to climate simulations in order to identify the principal effects of massive volcanic eruptions on the global climate.\nClimate scientists often refer to low-dimensional summarizations of climate data as “fingerprints”: these fingerprints may be motivated by domain expertise, e.g. global mean temperature, or by some statistical criterion, such as maximizing variance explained. Among their many uses in climate science, fingerprints are often used as inputs to Detection & Attribution studies; that is, they are used as evidence to detect changes in a climate quantity of interest and to attribute that change to a particular (posited) cause. In collaboration with a cross-disciplinary team from SNL, I am working to develop advanced fingerprinting techniques which maximize the statistical power of downstream Detection & Attribution studies. This work incorporates ideas from supervised dimension reduction, such as Partial Least Squares or Canonical Correlation Analysis (PLS/CCA), to find “most-attributable” fingerprints, i.e., those aspects of the climate that appear to exhibit the most robust evidence of change. This analysis poses serious inferential challenges-i.e., if we have found the ‘most different’ aspect of the climate, classical attribution studies are no longer statistically valid-but we believe that we will be able to address these as climate models also provide us with the ability to generate “null samples” from which we can estimate a suitable sampling distribution.\nI also maintain an ongoing collaboration with Sandia National Labs (SNL) in the area of large graph analytics. Our recent work focuses on the intersection of computational algorithms and statistical theory for exceptionally large graph problems, with a particular focus on the “seed set expansion” (or local community detection) problem. My SNL collaborators bring expertise in a range of applied math and engineering fields, including high-performance computing, numerical linear algebra, tensor analysis, and graph algorithms. This collaboration is particularly rewarding as we are able to work with SNL’s mission-focused analytics teams to identify insights and challenges that a purely-academic investigation may overlook.\nIf you would like to discuss any of these topics further, please dont hesitate to get in touch.\nMy work has been supported by grants and fellowships from the US National Science Foundation (NSF), the US Intelligence Community (US IC), and the Sandia National Laboratories Laboratory Directed Research and Development program (SNL LDRD). For more, see here."
  },
  {
    "objectID": "research.html#footnotes",
    "href": "research.html#footnotes",
    "title": "Research Interests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis latter goal overlaps with “explainable” or “interpretable” AI (XAI); to the degree there is a difference, XAI focuses more on explaining the results of existing prediction algorithms - which may be of standalone value as black-box predictors - while my work centers the knowledge generation aims. In general, the methods I focus on consider predictive power as evidence of a meaningful scientific relationship, not as the end goal.↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "At Zicklin School of Business\n\nSTA9890: Statistical Learning for Data Mining:\n\nSpring 2025\nSpring 2024\n\nSTA9750/OPR9750: Basic Software Tools for Data Analysis:\n\nFall 2025\nSpring 2025\nFall 2024\nSpring 2024\n\nSTA9715: Applied Probability:\n\nFall 2025\nFall 2024"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Information",
    "section": "",
    "text": "Email: michael.weylandt@baruch.cuny.edu\nPhone: (646) 312-3257\nOffice:\nNewman Vertical Campus 11-246\n1 Bernard Baruch Way (25th St @ Lexington Ave, SE Corner)\nNew York, NY 10010\nLike many folks, I continue to work remotely most days.\nIf you want to get in touch, please email me at michael.weylandt@baruch.cuny.edu. My office phone is not a reliable means of contacting me.\nIf you find any mistakes on this website, please file an issue here."
  },
  {
    "objectID": "publications/hepatoscore14.html",
    "href": "publications/hepatoscore14.html",
    "title": "HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk",
    "section": "",
    "text": "Abstract: Background and Aims: Therapeutic, clinical trial entry and stratification decisions for hepatocellular carcinoma (HCC) are made based on prognostic assessments, using clinical staging systems based on small numbers of empirically selected variables that insufficiently account for differences in biological characteristics of individual patients’ disease.\nApproach and Results: We propose an approach for constructing risk scores from circulating biomarkers that produce a global biological characterization of individual patient’s disease. Plasma samples were collected prospectively from 767 patients with HCC and 200 controls, and 317 proteins were quantified in a Clinical Laboratory Improvement Amendments–certified biomarker testing laboratory. We constructed a circulating biomarker aberration score for each patient, a score between 0 and 1 that measures the degree of aberration of his or her biomarker panel relative to normal, which we call HepatoScore. We used log-rank tests to assess its ability to substratify patients within existing staging systems/prognostic factors. To enhance clinical application, we constructed a single-sample score, HepatoScore-14, which requires only a subset of 14 representative proteins encompassing the global biological effects. Patients with HCC were split into three distinct groups (low, medium, and high HepatoScore) with vastly different prognoses (medial overall survival 38.2/18.3/7.1 months; P &lt; 0.0001). Furthermore, HepatoScore accurately substratified patients within levels of existing prognostic factors and staging systems (P &lt; 0.0001 for nearly all), providing substantial and sometimes dramatic refinement of expected patient outcomes with strong therapeutic implications. These results were recapitulated by HepatoScore-14, rigorously validated in repeated training/test splits, concordant across Myriad RBM (Austin, TX) and enzyme-linked immunosorbent assay kits, and established as an independent prognostic factor.\nConclusions: HepatoScore-14 augments existing HCC staging systems, dramatically refining patient prognostic assessments and therapeutic decision making and enrollment in clinical trials. The underlying strategy provides a global biological characterization of disease, and can be applied broadly to other disease settings and biological media.\nPublisher DOI: 10.1002/hep.31555\nPubMed: 32931023\n\nCitation:\n@ARTICLE{Morris:2021,\n  AUTHOR=\"J.S. Morris and M.M. Hassan and Y.E. Zohner and Z. Wang and L. Xiao and A. Rashid and A. Haque and R. Abdel-Wahad and Y.A. Mohamed and K.L. Ballard and R.A. Wolff and B. George and L. Li and G.I. Allen and M. Weylandt and D. Li and W. Wang and K. Raghav and J. Yao and H.M. Amin and A.O. Kaseb\",\n  TITLE=\"HepatoScore14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk\",\n  JOURNAL=\"Hepatology\",\n  VOLUME=73,\n  NUMBER=6,\n  PAGES=\"2278--2292\",\n  YEAR=2021,\n  DOI=\"10.1002/hep.31555\"\n}"
  },
  {
    "objectID": "publications/thesis.html",
    "href": "publications/thesis.html",
    "title": "Computational and Statistical Methodology for Highly Structured Data",
    "section": "",
    "text": "Abstract: Modern data-intensive research is typically characterized by large scale data and the impressive computational and modeling tools necessary to analyze it. Equally important, though less remarked upon, is the important structure present in large data sets. Statistical approaches that incorporate knowledge of this structure, whether spatio-temporal dependence or sparsity in a suitable basis, are essential to accurately capture the richness of modern large scale data sets. This thesis presents four novel methodologies for dealing with various types of highly structured data in a statistically rich and computationally efficient manner. The first project considers sparse regression and sparse covariance selection for complex valued data. While complex valued data is ubiquitous in spectral analysis and neuroimaging, typical machine learning techniques discard the rich structure of complex numbers, losing valuable phase information in the process. A major contribution of this project is the development of convex analysis for a class of non-smooth “Wirtinger” functions, which allows high-dimensional statistical theory to be applied in the complex domain. The second project considers clustering of large scale multi-way array (“tensor”) data. Efficient clustering algorithms for convex bi-clustering and co-clustering are derived and shown to achieve an order-of-magnitude speed improvement over previous approaches. The third project considers principal component analysis for data with smooth and/or sparse structure. An efficient manifold optimization technique is proposed which can flexibly adapt to a wide variety of regularization schemes, while efficiently estimating multiple principal components. Despite the non-convexity of the manifold constraints used, it is possible to establish convergence to a stationary point. Additionally, a new family of “deflation” schemes are proposed to allow iterative estimation of nested principal components while maintaining weaker forms of orthogonality. The fourth and final project develops a multivariate volatility model for US natural gas markets. This model flexibly incorporates differing market dynamics across time scales and different spatial locations. A rigorous evaluation shows significantly improved forecasting performance both in- and out-of-sample. All four methodologies are able to flexibly incorporate prior knowledge in a statistically rigorous fashion while maintaining a high degree of computational performance.\nCopy of Record: Rice University Library\nSummary: This thesis collects most of the work I published during my Ph.D. as well as some work on statistical learning in complex variables which has not yet been published elsewhere. The treatment of complex variables is slightly different than that typically appearing in the literature: by tweaking the usual construction of \\(\\mathbb{C}^n\\) as a real inner product space, I show that essentially all results from real-valued statistical learning theory translate without effort: results for complex-valued sparse regression or Gaussian graphical models are comparable to their real-valued, with a better sample complexity if we assume “proper” (isotropic) sub-Gaussian noise. These results can be combined with the concentration bounds of Fiecas et al (2019, EJS) to get best-in-class sparsistency results for probabilistic graphical models of stochastic processes. While the optimization theory developed in this thesis could be pushed further, e.g. to non-Gaussian / generalized linear models, the correct construction of complex-GLMs is a bit tricky1 and omitted here.\nPresentations: Thesis defense was held virtually and the Zoom recording can be viewed below:\nDirect YouTube Link. Slides can be found here.\nCitation:"
  },
  {
    "objectID": "publications/thesis.html#footnotes",
    "href": "publications/thesis.html#footnotes",
    "title": "Computational and Statistical Methodology for Highly Structured Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome definitional problems arise when mapping complex covariates to the real-valued linear predictor: it’s not clear how to construct monotonicity here.↩︎"
  },
  {
    "objectID": "publications/pca+.html",
    "href": "publications/pca+.html",
    "title": "Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints",
    "section": "",
    "text": "Abstract: Dimension reduction techniques are an essential part of the climate analyst’s toolkit. Due to the enormous scale of climate data, dimension reduction methods are used to identify major patterns of variability within climate dynamics, to create compelling and informative visualizations, and to quantify major named modes such as El Niño–Southern Oscillation. Principal components analysis (PCA), also known as the method of empirical orthogonal functions (EOFs), is the most commonly used form of dimension reduction, characterized by a remarkable confluence of attractive mathematical, statistical, and computational properties. Despite its ubiquity, PCA suffers from several difficulties relevant to climate science: high computational burden with large datasets, decreased statistical accuracy in high dimensions, and difficulties comparing across multiple datasets. In this paper, we introduce several variants of PCA that are likely to be of use in climate sciences and address these problems. Specifically, we introduce non-negative, sparse, and tensor PCA and demonstrate how each approach provides superior pattern recognition in climate data. We also discuss approaches to comparing PCA-family results within and across datasets in a domain-relevant manner. We demonstrate these approaches through an analysis of several runs of the E3SM climate model from 1991 to 1995, focusing on the simulated response to the Mt. Pinatubo eruption; our findings are consistent with a recently identified stratospheric warming fingerprint associated with this type of stratospheric aerosol injection.\nPublisher DOI: 10.1175/JCLI-D-23-0267.1\nSupporting Materials: Supporting materials, including code necessary to reproduce our experiments, can be found at 10.5281/zenodo.10581709\n\nCitation:\n@ARTICLE{Weylandt:2024-BeyondPCA,\n  TITLE=\"Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints\",\n  AUTHOR=\"Michael Weylandt and Laura P. Swiler\",\n  JOURNAL=\"Journal of Climate\",\n  YEAR=2024,\n  VOLUME=37,\n  NUBER=5,\n  DOI=\"10.1175/JCLI-D-23-0267.1\",\n  PAGES={1723–1735 }\n}"
  },
  {
    "objectID": "publications/conga.html",
    "href": "publications/conga.html",
    "title": "Sparse Partial Least Squares for Coarse Noisy Graph Alignment",
    "section": "",
    "text": "Abstract: Graph signal processing (GSP) provides a powerful framework for analyzing signals arising in a variety of domains. In many applications of GSP, multiple network structures are available, each of which captures different aspects of the same underlying phenomenon. To integrate these different data sources, graph alignment techniques attempt to find the best correspondence between vertices of two graphs. We consider a generalization of this problem, where there is no natural one-to-one mapping between vertices, but where there is correspondence between the community structures of each graph. Because we seek to learn structure at this higher community level, we refer to this problem as “coarse” graph alignment. To this end, we propose a novel regularized partial least squares method which both incorporates the observed graph structures and imposes sparsity in order to reflect the underlying block community structure. We provide efficient algorithms for our method and demonstrate its effectiveness in simulations.\nPublisher DOI: 10.1109/SSP49050.2021.9513753\nWorking Copy: ArXiv 2104.02810\n\nSummary: We consider the problem of aligning two graphs on different node sets, e.g., Facebook and Twitter. Many users may be represented on both networks, but there are also a large number of users who use only one social media platform. Solving this problem exactly entails a (generalization) of the NP-hard graph-alignment problem, so we consider a relaxation in which we seek to find corresponding communities (e.g., basset hound enthusiasts) across both networks. To do so, we assume that common signals are observed on both networks (e.g., interest in a particular celebrity dog) and use that signal to align communities. Because this approach aligns communities rather than nodes and does so using (potentially noisy) signals, we refer to the resulting problem as the CONGA problem (Coarse Noisy Graph Alignment).\n\n\n\nCONGA Generative Model: Common Signals Excite Corresponding Communities\n\n\nWe proceed by constructing the signal correlation matrix across the two graphs and performing a regularized SVD on this inner product:\n\\[\\begin{align*}\n    \\text{arg max}_{\\mathbf{U}, \\mathbf{V}} & \\text{Tr}(\\mathbf{U}^{\\top}\\mathbf{X}_1^\\top\\mathbf{X}_2\\mathbf{V}) - \\lambda_1P_1(\\mathbf{U}) - \\lambda_2P_2(\\mathbf{V})  \\\\\n    \\text{subject to }&{\\mathbf{U} \\in \\textsf{conv}\\,\\mathcal{V}^{\\mathbf{I}_{n_1} + \\alpha_1\\mathbf{L}_1}_{n_1 \\times K}, \\mathbf{V} \\in \\textsf{conv}\\,\\mathcal{V}^{\\mathbf{I}_{n_2} + \\alpha_2\\mathbf{L}_2}_{n_2 \\times K}}\n\\end{align*}\\]\nThis approach builds on my work on multi-rank sparse and functional PCA and extends it to the partial least squares context. The use of the flexible SFPCA regularization framework allows us to simultaneously achieve four goals:\n\nEach community is supported on a sparse set of graph 1 nodes (\\(P_1(\\mathbf{U})\\))\nEach community is supported on a sparse set of graph 2 nodes (\\(P_2(\\mathbf{V})\\))\nEach community is well clustered with respect to graph 1, i.e., each community is smooth with respect to the Laplacian \\(\\mathbf{L}_1\\)\nEach community is well clustered with respect to graph 2, i.e., each community is smooth with respect to the Laplacian \\(\\mathbf{L}_2\\)\n\nThe resulting estimator is computationally efficient and performs well in simulation, as shown in our paper. We hope to further study this model theoretically, but correlated sampling of graphs on different node sets is a tricky and open problem. If you have particular expertise in this area, please do get in touch!\nPresentations: I presented on this work at SSP 2021, which was held virtually. A recording of my presentation can be found below:\n\n\nView on YouTube. Slides from this talk can be found here.\n\nCitation:\n@INPROCEEDINGS{Weylandt:2021-CONGA,\n  AUTHOR=\"Michael Weylandt and George Michailidis and T. Mitchell Roddenberry\",\n  TITLE=\"Sparse Partial Least Squares for Coarse Noisy Graph Alignment\",\n  DOI=\"10.1109/SSP49050.2021.9513753\"\n  PAGES={561-565},\n  CROSSREF=\"SSP:2021\"\n}\n\n@PROCEEDINGS{SSP:2021,\n  BOOKTITLE=\"SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop\",\n  YEAR=2021,\n  LOCATION=\"Rio de Janeiro, Brazil\"\n  EDITOR=\"Jose Carlos Moreira Bermudez and Vitor H. Nascimento\"\n}"
  },
  {
    "objectID": "publications/castle.html",
    "href": "publications/castle.html",
    "title": "Space-Time Causal Discovery in Climate Science: A Local Stencil Learning Approach",
    "section": "",
    "text": "Abstract: Causal discovery tools enable scientists to infer meaningful relationships from observational data, spurring advances in fields as diverse as biology, economics, and climate science. Despite these successes, the application of causal discovery to space-time systems remains immensely challenging due to the high-dimensional nature of the data. For example, in climate sciences, modern observational temperature records over the past few decades regularly measure thousands of locations around the globe. To address these challenges, we introduce Causal Space-Time Stencil Learning (CaSTLe), a novel algorithm for discovering causal structures in complex space-time systems. CaSTLe leverages regularities in local dependence to learn governing global dynamics. This local perspective eliminates spurious confounding and drastically reduces sample complexity, making space-time causal discovery practical and effective. These advances enable causal discovery of geophysical phenomena that were previously unapproachable, including non-periodic, transient phenomena such as volcanic eruption plumes. When applied to ever-larger spatial grids, CaSTLe’s performance actually improves because it transforms large grids into informative spatial replicates. We successfully apply CaSTLe to discover the atmospheric dynamics governing the climate response to the 1991 Mount Pinatubo volcanic eruption. We additionally provide extensive validation experiments to demonstrate the effectiveness of CaSTLe over existing causal-discovery frameworks on a range of climate-inspired benchmarks.\nWorking Copy (ESS Open Archive): 10.22541/essoar.172253117.78663487\n\nSummary: We introduce a new method for learning the dynamics of causal systems, that is, the physical laws that define their behavior. While this problem, causal discovery, is not new, existing tools are ill-suited for large climate datasets. Current state-of-the-art approaches use statistical techniques to search for causal relationships between all aspects of a system, examining billions of possible causal effects. Instead of this ‘needle-in-a-haystack’ search, we incorporate basic physical principles—requiring effects to be ‘local’ and ‘uniform’—to massively simplify the causal discovery problem. We show that our approach can be used to make important climate discoveries by analyzing the 1991 Mt. Pinatubo eruption.\n\n\n\nIllustration of the CaSTle Process: Transformation from original domain to reduced coordinates, parent identification in the reduced space, and re-expansion to the original domain.\n\n\n\n\n\nApplication of CaSTLe to the 1991 Mt. Pinatubo Eruption - CaSTLe successfully identifies drivers of stratospheric zonal transport (early) and meridional transport (later). See paper for details.\n\n\n\nCitation:\n@ARTICLE{Nichol:2024,\n  AUTHOR=\"Jake J. Nichol and Michael Weylandt and G. Matthew Fricke and Melanie E. Moses and Diana L. Bull and Laura P. Swiler\",\n  TITLE=\"Space-Time Causal Discovery in Climate Science: A Local Stencil Learning Approach\",\n  YEAR=2024,\n  DOI={10.22541/essoar.172253117.78663487},\n  JOURNAL=\"ESS Open Archive 172253117.78663487\"\n}"
  },
  {
    "objectID": "publications/luis.html",
    "href": "publications/luis.html",
    "title": "A Tutorial on Hidden Markov Models using Stan",
    "section": "",
    "text": "Abstract: We implement a standard Hidden Markov Model (HMM) and the Input-Output Hidden Markov Model for unsupervised learning of time series dynamics in Stan. We begin by reviewing three commonly-used algorithms for inference and parameter estimation, as well as a number of computational techniques and modeling strategies that make full Bayesian inference practical. For both models, we demonstrate the effectiveness of our proposed approach in simulations. Finally, we give an example of embedding a HMM within a larger model using an example from the econometrics literature.\nPublisher’s DOI: 10.5281/zenodo.1284341\n\nSummary: This work motivates and demonstrates the use of Bayesian inference for Hidden Markov Models, with a eye towards their use in the probabilistic programming framework Stan. We give examples of both classical HMMs as well as several more advanced variants. The work in this talk has largely been superseded by advances in Stan itself, including an built-in HMM framework in Stan, version 2.24: new work should proceed from the material in official Stan documentation or from the semi-official user-contributed tutorial.\n\nCitation:\n``` @INPROCEEDINGS{Damiano:2018, AUTHOR=“Luis Damiano and Brian Peterson and Michael Weylandt”, TITLE=“A Tutorial on Hidden {M}arkov Models Using {S}tan” DOI=“10.5281/zenodo.1284341”, CROSSREF={StanCon:2018} }\n@PROCEEDINGS{StanCon:2018, LOCATION=“Asilomar, CA”, BOOKTITLE=“StanCon 2018: Proceedings of the 2018 Stan Users’ Conference”, URL=“https://mc-stan.org/events/stancon2018/” }"
  },
  {
    "objectID": "publications/sfpca.html",
    "href": "publications/sfpca.html",
    "title": "Sparse and Functional Principal Components Analysis",
    "section": "",
    "text": "Abstract: Regularized variants of Principal Components Analysis, especially Sparse PCA and Functional PCA, are among the most useful tools for the analysis of complex high-dimensional data. Many examples of massive data, have both sparse and functional (smooth) aspects and may benefit from a regularization scheme that can capture both forms of structure. For example, in neuro-imaging data, the brain’s response to a stimulus may be restricted to a discrete region of activation (spatial sparsity), while exhibiting a smooth response within that region. We propose a unified approach to regularized PCA which can induce both sparsity and smoothness in both the row and column principal components. Our framework generalizes much of the previous literature, with sparse, functional, two-way sparse, and two-way functional PCA all being special cases of our approach. Our method permits flexible combinations of sparsity and smoothness that lead to improvements in feature selection and signal recovery, as well as more interpretable PCA factors. We demonstrate the efficacy of our method on simulated data and a neuroimaging example on EEG data.\nPublisher DOI: 10.1109/DSW.2019.8755778\nWorking Copy: ArXiv 1309.2895\n\nSummary: We give a unified proposal for regularized PCA, allowing for both sparsity and smoothness (functional structure) in both the estimated observation and feature embeddings (left and right singular values): [{ ^n{{}}, ^p{{}}} ^T - {} P_{}() - {} P{}()] Our approach incorporates a variety of prior proposals as special cases, including:\n\nClassical (Unregularized) PCA\nOne-Way Sparse PCA: Shen and Huang (J. Multivariate Analysis, 2008)\nTwo-Way Sparse PCA: Allen, Grosenick, and Taylor (J. American Statistical Association, 2014) and Witten, Tibshirani, and Hastie (Biostatistics, 2009)\nOne-Way Functional PCA: Silverman (Annals of Statistics, 1996) and Huan, Shen, and Buja (Electronic Journal of Statistics, 2008)\nTwo-Way Functional PCA: Huang, Shen, and Buja (J. American Statistical Association, 2009)\n\nWe prove that our proposal is well-formulated, i.e., that all constraints are either binding or clearly slack with no masking, and give an efficient algorithm for finding a Nash point of the non-convex SFPCA problem. Finally, we apply our result in simulations and to EEG data and show that it finds meaningful and useful principal components.\nThe ArXiv-only appendices of this paper have quite a lot of useful content, including detailed analysis of the SFPCA problem and equivalencies to other regularized PCA schemes. Appendix C gives a thorough review of the regularized PCA literature as of 2019.\n\n\n\nIn Silico Comparison of SFPCA with other PCA Variants\n\n\n\n\n\nRecovering space-time localized left and right singular vectors with SFPCA\n\n\nRelated Software: MoMA\n\nCitation:\n@INPROCEEDINGS{Allen:2019,\n  TITLE=\"Sparse and Functional Principal Components Analysis\",\n  AUTHOR=\"Genevera I. Allen and Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755778\",\n  PAGES={11-16}\n}\n\n@PROCEEDINGS{DSW:2019,\n  TITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}"
  },
  {
    "objectID": "publications/fair_pca.html",
    "href": "publications/fair_pca.html",
    "title": "Debiasing Projections for Fair Principal Components Analysis",
    "section": "",
    "text": "Abstract: Machine learning models can often reflect or exacerbate societal biases present in training data. While most attention in the nascent field of algorithmic fairness has focused on bias mitigation for supervised learning, biases can also be a problem for unsupervised analyses. With Principal Components Analysis (PCA), for example, biased dimension reduction can affect downstream analysis as well as affect data exploration and modeling decisions. In this paper, we propose a novel framework for Fair PCA that finds debiasing projections of the data, or projections that remove the effect from any protected attribute. We show that our approach has many advantages over existing formulations of Fair PCA including a fast, closed form solution, generality and wide applicability to many protect attributes, and its ease of interpretation and usage like projections in classical PCA. We demonstrate the bias mitigating advantages of our Fair PCA method compared to existing proposals for Fair PCA on several synthetic and benchmark datasets."
  },
  {
    "objectID": "publications/ng_volatility.html",
    "href": "publications/ng_volatility.html",
    "title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating Futures Market Realized Volatility",
    "section": "",
    "text": "Winner of the ASA Section on Business and Economic Statistics (B&E) 2020 Student Paper Competition\nAbstract: Financial markets for natural gas are an important and rapidly-growing segment of commodities markets. Like other commodities markets, there is an inherent spatial structure to natural gas markets, with different price dynamics for different points of delivery hubs. Certain hubs support highly liquid markets, allowing efficient and robust price discovery, while others are highly illiquid, limiting the effectiveness of standard risk management techniques. We propose a joint modeling strategy, which uses high-frequency information from thickly-traded hubs to improve volatility estimation and risk management at thinly-traded hubs. The resulting model has superior in- and out-of-sample predictive performance, particularly for several commonly used risk management metrics, demonstrating that joint modeling is indeed possible and useful. To improve estimation, a Bayesian estimation strategy is employed and data-driven weakly informative priors are suggested. Our model is robust to sparse data and can be effectively used in any market with similar irregular patterns of data availability.\nWorking Copy: ArXiv 1907.10152\n\nSummary: In this work, we built a new model for risk management (volatility forecasting) of US Natural Gas markets. These markets are interesting in that they combine a overall “market” effect with location-specific volatility. We adapt the “Realized Beta GARCH” model of Hansen, Lunde, and Voev (J. Applied Econometrics, 2014) to this problem, combining high-frequency data from NYSE futures markets with daily data from geographically diverse spot markets. We give a Bayesian analysis of this problem, developing realistic priors, thoroughtly evaluating MCMC sampler performance, and rigorously validating the out-of-sample forecast accuracy of our approach. The specific findings of our analysis are less of interest - and likely to be out of date anyways - but the analytical framework and evaluation methodology are rigorous and can be easily applied to other risk management problems.\n\n\n\nWAIC Evaluation with Outlier Detection\n\n\n\n\n\nOut-of-Sample Volatility Forecast Validation"
  },
  {
    "objectID": "publications/trout.html",
    "href": "publications/trout.html",
    "title": "Automatic Registration and Convex Clustering of Time Series",
    "section": "",
    "text": "Abstract: Clustering of time series data exhibits a number of challenges not present in other settings, notably the problem of registration (alignment) of observed signals. Typical approaches include pre-registration to a user-specified template or time warping approaches which attempt to optimally align series with a minimum of distortion. For many signals obtained from recording or sensing devices, these methods may be unsuitable as a template signal is not available for pre-registration, while the distortion of warping approaches may obscure meaningful temporal information. We propose a new method for automatic time series alignment within a clustering problem. Our approach, Temporal Registration using Optimal Unitary Transformations (TROUT), is based on a novel dissimilarity measure between time series that is easy to compute and automatically identifies optimal alignment between pairs of time series. By embedding our new measure in a optimization formulation, we retain well-known advantages of computational and statistical performance. We provide an efficient algorithm for TROUT-based clustering and demonstrate its superior performance over a range of competitors.\nPublisher DOI: 10.1109/ICASSP39728.2021.9414417\nWorking Copy: ArXiv 2012.04756\n\nSummary: Clustering time series is a common problem in many applications, but it is much more difficult when analyzing non-aligned signals. Typically, signals are pre-registered to some template or common alignment, but this assumes that such a template exists, when discovering “standard signals” is often the goal of clustering in the first case. We instead propose to align signals to each other within the clustering problem. Given two signals, the optimal alignment between them can be found by an orthogonal rotation (phase-shift), with closed form: this motivates TROUT-clustering [{ ^{n p}} d{}(, )^2 + {}^n w{ij} |{i} - {j}|q] where [d{}(, ) = _{To solve this problem, we develop a Majorization-Minimization scheme, the sub-problems of which have closed form solution, yielding an efficient algorithm.\nNote that the clustering formulation is designed so that each observation (row of \\(\\mathbf{X}\\)) is rotated to coincide with the shared centroids (row of \\(\\mathbf{U}\\)) rather than to each other: this avoids the registration problem entirely, but does give a solution that is only identified up to phase. Our method performs well under a variety of noise conditions and often even out-performs traditional clustering methods with oracle pre-registration.\n\n\n\nExamples of TROUT Clustering\n\n\n\n\n\nComparison of TROUT, Time-Warping, and Traditional Clustering\n\n\n\nCitation:\n@INPROCEEDINGS{Weylandt:2021-TROUT,\n  AUTHOR=\"Michael Weylandt and George Michailidis\",\n  TITLE=\"Automatic Registration and Convex Clustering of Time Series\",\n  DOI=\"10.1109/ICASSP39728.2021.9414417\"\n  PAGES={5609-5613},\n  CROSSREF=\"ICASSP:2021\"\n}\n\n@PROCEEDINGS{ICASSP:2021,\n  BOOKTITLE=\"ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing\",\n  YEAR=2021,\n  LOCATION=\"Toronto, Canada\"\n  EDITOR=\"Tim Davidson and Dong Yu\"\n}"
  },
  {
    "objectID": "software/clustRviz.html",
    "href": "software/clustRviz.html",
    "title": "clustRviz",
    "section": "",
    "text": "The clustRviz R package implements the CARP and CBASS algorithms for computing the entire convex clustering solution path. In addition to the path-wise solvers, the package also implements efficient algorithms for solving the convex clustering and bi-clustering problems at a fixed grid of penalty parameters. The package also implements a novel method for simultaneous clustering and imputation, making it one of the few pieces of clustering software that can trnasparently handle missing values.\nDirect Link: http://github.com/DataSlingers/clustRviz\nPackage Documentation: https://DataSlingers.github.io/clustRviz/\nRelated Publications: Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization"
  },
  {
    "objectID": "software/sgdnet.html",
    "href": "software/sgdnet.html",
    "title": "sgdnet",
    "section": "",
    "text": "The sgdnet R package implements a stochastic gradient method for elastic-net penalized generalized linear models. The package is stable and correct, but performance is not quite what we hoped to attain. Researchers seeking to implement stochastic gradient methods for regularized linear models are advised to take advantage of modern deep learning frameworks and massively parallel computational hardware instead.\nDirect Link: https://github.com/jolars/sgdnet\nPackage Documentation: http://jolars.github.io/sgdnet/"
  },
  {
    "objectID": "software/exlasso.html",
    "href": "software/exlasso.html",
    "title": "ExclusiveLasso",
    "section": "",
    "text": "The ExclusiveLasso R package implements the Exclusive Lasso penalty as analyzed by Campbell and Allen (EJS, 2017), for Gaussian, logistic, and Poisson regression. The interface is glmnet compatible and the package supports both proximal gradient and coordinate descent solvers.\nThe package vignette is also notable for a thorough derivation of the back-tracking proximal Newton algorithm used for penalized GLMs, giving more detial than I have found elsewhere. Reading it would be useful for 1st or 2nd year Ph.D. students interested in efficient implementation of this family of algorithms.\nDirect Link: http://github.com/DataSlingers/ExclusiveLasso\nPackage Documentation: https://DataSlingers.github.io/ExclusiveLasso/"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Below is a list of my publications: click through the linked title for more details."
  },
  {
    "objectID": "publications.html#submitted",
    "href": "publications.html#submitted",
    "title": "Publications",
    "section": "Submitted",
    "text": "Submitted\n\nC.R. Wentland, M. Weylandt, L.P. Swiler, T.S. Ehrmann, and D.Bull “Conditional multi-step attribution for climate forcings” 2024.\nJ.J. Nichol, M. Weylandt, G.M. Fricke, M. Moses, D. Bull, and L.P. Swiler. “Causal Spatiotemporal Stencil Learning: Local Causal Dynamics in Complex Systems.” 2024.\nR. B. Lehoucq, M. Weylandt, and J. W. Berry. “Optimal accuracy for linear sets of equations with the graph Laplacian”. 2024.\nC. O. Little†, M. Weylandt† and G. I. Allen. “To the Fairness Frontier and Beyond: Identifying, Quantifying, and Optimizing the Fairness-Accuracy Pareto Frontier”. 2022.\nM. Weylandt and G. Michailidis. “Multivariate Analysis for Multiple Network Data via Semi-Symmetric Tensor PCA”. 2022.\nM. Weylandt and G. I. Allen. “Debiasing Projections for Fair Principal Components Analysis”. 2021.\nM. Navarro, G. I. Allen, and M. Weylandt. “Network Clustering for Latent State and Changepoint Detection”. 2021.\nM. Weylandt, Y. Han, and K. B. Ensor. “Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating Futures Market Realized Volatility”. 2019.\nWinner of the ASA Section on Business and Economic Statistics (B&E) 2020 Student Paper Competition\nA.P. Drager, M. Weylandt, G. Chuyong, D. Kenfack, D.W. Thomas, and A.E. Dunham. “Ecological correlates of reproductive status in a guild of Afrotropical understory trees.” 2019."
  },
  {
    "objectID": "publications.html#published",
    "href": "publications.html#published",
    "title": "Publications",
    "section": "Published",
    "text": "Published\n\nM. Weylandt and L. P. Swiler. “Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints.” Journal of Climate 37(5), pp.1723-1735. 2024.\nM. Weylandt, G. Michailidis, and T. M. Roddenberry. “Sparse Partial Least Squares for Coarse Noisy Graph Alignment.” SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop 2021, pp.561-565. 2021.\nM. Weylandt, T.M. Roddenberry, and G. I. Allen. “Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering.” DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021. pp.1-8. 2021.\nM. Weylandt and G. Michailidis. “Automatic Registration and Clustering of Time Series.” ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, pp.5609-5613. 2021.\nJ.S. Morris, M.M. Hassan, Y.E. Zohner, Z. Wang, L. Xiao, A. Rashid, A. Haque, R. Abdel-Wahad, Y.A. Mohamed, K.L. Ballard, R.A. Wolff, B. George, L. Li, G. I. Allen, M. Weylandt, D. Li, W. Wang, K. Raghav, J. Yao, H.M. Amin, and A.O. Kaseb. “HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk.” Hepatology 73(6), pp. 2278-2292. 2021.\nM. Weylandt. “Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques.” CAMSAP 2019: Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pp.500-504. 2019.\nM. Weylandt, J. Nagorski, and G. I. Allen. “Interactive Visualizations and Fast Computation for Convex Clustering via Algorithmic Regularization.” Journal of Computational and Graphical Statistics 29(1), pp. 87-96. 2020.\nWinner of the ASA Section on Statistical Learning and Data Science (SLDS) 2019 Student Paper Competition\nM. Weylandt. “Splitting Methods for Convex Bi-Clustering and Co-Clustering.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.237-244. 2019.\nG. I. Allen and M. Weylandt. “Sparse and Functional Principal Components Analysis.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.11-16. 2019."
  },
  {
    "objectID": "publications.html#other-professional-writing",
    "href": "publications.html#other-professional-writing",
    "title": "Publications",
    "section": "Other Professional Writing",
    "text": "Other Professional Writing\n\nM. Weylandt. “Computational and Statistical Methodology for Highly-Structured Data.” Ph.D. Thesis, Rice University. 2020.\nL. Damiano, B. Peterson, and M. Weylandt. “A Tutorial on Hidden Markov Models using Stan.” StanCon 2018.\n\n\nA BibTeX file for these publications can be found here."
  },
  {
    "objectID": "funding.html",
    "href": "funding.html",
    "title": "Research Support and Funding",
    "section": "",
    "text": "My research has been supported by grants and fellowships from the US National Science Foundation (NSF), the US Intelligence Community (US IC), and the Sandia National Laboratories Laboratory Directed Research and Development program (SNL LDRD).\nCurrent Funding"
  },
  {
    "objectID": "funding.html#footnotes",
    "href": "funding.html#footnotes",
    "title": "Research Support and Funding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSubcontract to CLDERA; CLDERA PI: D. Bull (SNL).↩︎"
  }
]