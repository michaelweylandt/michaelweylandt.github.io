{
  "articles": [
    {
      "path": "about.html",
      "title": "About Me",
      "author": [],
      "contents": "\nMichael Weylandt is currently an Intelligence Community Postdoctoral Fellow, working with George Michailidis at the University of Florida. His work focuses on statistical machine learning theory for structured multivariate time series, viewed through a graphical model lens. His work has been recognized with best paper awards from the American Statistical Association in both Statistical Learning and Data Science and in Business & Economic Statistics. He has served as a mentor in the Google Summer of Code program for 7 years on behalf of the R Foundation for Statistical Computing and previously held an NSF Graduate Research Fellowship. Prior to beginning his Ph.D. studies, he worked at Morgan Stanley as a quantitative analyst, focusing on derivatives pricing and financial risk management. He received a Bachelor’s of Science in Engineering from Princeton University in 2008 and a Ph.D. in Statistics from Rice University in 2020.\nFor more details, see my Full CV.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:44-06:00"
    },
    {
      "path": "contact.html",
      "title": "Contact Information",
      "author": [],
      "contents": "\nLike most folks, I continue to work remotely most days. If you want to get in touch, please email me at michael.weylandt@ufl.edu or give me a call at 713-204-9366.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:44-06:00"
    },
    {
      "path": "index.html",
      "title": "Welcome!",
      "author": [],
      "contents": "\nHello and welcome to my website! My name is Michael Weylandt. I am currently an US Intelligence Community Postdoctoral Research Fellow hosted at the University of Florida Informatics Institute, where I am advised by George Michailidis as well as two mentors from the Intelligence Community. Before this, I earned my Ph.D. at Rice University in 2020, worked in finance from 2013 to 2015, and taught at Sherborne School in the UK from 2012 to 2013. My research is in statistical machine learning, with a recent focus on network data and in machine learning fairness. For a more formal bio, please see my CV and my About page. More detail on my Publications and Software can be found on those pages.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:44-06:00"
    },
    {
      "path": "publication_ccpd.html",
      "title": "Coupled-CP Decomposition for Principal Components Analysis of Symmetric Networks",
      "description": "Submitted article (under review)",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "George Michailidis",
          "url": {}
        }
      ],
      "contents": "\nAbstract: In a number of application domains, one observes a sequence of network data; for example, repeated measurements between users interactions in social media platforms, financial correlation networks over time, or across subjects, as in multi-subject studies of brain connectivity. One way to analyze such data is by stacking networks into a third-order array or tensor. We propose a principal components analysis (PCA) framework for sequence network data, based on a novel decomposition for semi-symmetric tensors. We derive efficient algorithms for computing our proposed “Coupled CP” decomposition and establish estimation consistency of our approach under an analogue of the spiked covariance model with rates the same as the matrix case up to a logarithmic term. Our framework inherits many of the strengths of classical PCA and is suitable for a wide range of unsupervised learning tasks, including identifying principal networks, isolating meaningful changepoints or outliers across observations, and for characterizing the “variability network” of the most varying edges. Finally, we demonstrate the effectiveness of our proposal on simulated data and on examples from political science and financial economics. The proof techniques used to establish our main consistency results are surprisingly straight-forward and may find use in a variety of other matrix and tensor decomposition problems.\nWorking Copy: ArXiv 2202.04719\nSummary: We develop and approach for performing Principal Components Analysis (PCA) on network series data - sets of networks observed on the same node set. Network series data are observed when a changing network is observed over time (e.g., a social media network at the end of each week) or when a statistical network is estimated from time series data (e.g., stock market correlation networks in different years). We approach this problem by embedding the network series in a semi-symmetric tensor and performing tensor PCA on this representation.\nSchematic Diagram of Semi-Symmetric Tensor PCAWe rigorously analyze Tensor PCA in the semi-symmetric context, proving consistency under an analogue of the “low-rank + noise” model for matrix PCA: somewhat remarkably, despite the difficulty of the tensor setting, our results are within a logarithmic factor of classical PCA results. Our proof technique depends on the classical Davis-Kahan theorem (and some painful algebra) and we hope to apply it further to a variety of tensor decomposition problems. Finally, we apply our method to a variety of synthetic and real data sets and find some counter-intuitive results about the Supreme Court of the United States.\nTensor PCA applied to SCOTUS Voting Behaviors\n\n\n",
      "last_modified": "2022-02-15T18:01:44-06:00"
    },
    {
      "path": "publication_clustRviz.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:45-06:00"
    },
    {
      "path": "publication_coclustering_algs.html",
      "title": "Splitting Methods for Convex Bi-Clustering and Co-Clustering",
      "description": "*DSW 2019: Proceedings of the IEEE Data Science Workshop 2019*, pp.237-244. 2019.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Co-Clustering, the problem of simultaneously identifying clusters across multiple aspects of a data set, is a natural generalization of clustering to higher-order structured data. Recent convex formulations of bi-clustering and tensor co-clustering, which shrink estimated centroids together using a convex fusion penalty, allow for global optimality guarantees and precise theoretical analysis, but their computational properties have been less well studied. In this note, we present three efficient operator-splitting methods for the convex co-clustering problem: a standard two-block ADMM, a Generalized ADMM which avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests, and experimental evidence confirms, that the Generalized ADMM is far more efficient for large problems.\nPublisher DOI: 10.1109/DSW.2019.8755599\nWorking Copy: ArXiv 1901.06075\nSummary: Chi and Lange (J. Computational and Graphical Statistics, 2015) established that splitting methods are an easy-to-implement highly-performant way to solve convex clustering problems. In this work, I extend their analysis to the general case of tensor co-clustering, considering three operator-splitting schemes:\nThe classic ADMM\nA generalized (quadratically perturbed) ADMM\nDavis-Yin splitting (equivalent to AMA for this problem)\nClassical ADMM has the best per iteration performance, but requires solving a (tensor) Sylvester equation at each iteration, which is typically prohibitive. To avoid this, I construct a generalized ADMM scheme that avoids any matrix decompositions or inverses in the updates.\n\\[\\begin{align*}\n  \\mathbf{U}^{(k+1)} &= \\left(\\alpha \\mathbf{U}^{(k)} + \\mathbf{X} + \\rho\\mathbf{D}_{\\text{row}}^T(\\mathbf{V}^{(k)} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}} - \\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k)}) \\right. \\\\ &\\left.\\qquad+ \\rho(\\mathbf{V}^{(k)}_{\\text{col}} - \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}} - \\mathbf{U}^{(k)}\\mathbf{D}_{\\text{col}})\\mathbf{D}_{\\text{col}}^T \\right)/(1+\\alpha) \\\\\n\\begin{pmatrix} \\mathbf{V}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{V}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{row}, q}}(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{row}}) \\\\\n  \\text{prox}_{\\lambda / \\rho  \\|\\cdot\\|_{\\text{col}, q}}(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} + \\rho^{-1}\\mathbf{Z}^{(k)}_{\\text{col}})  \\end{pmatrix} \\\\\n    \\begin{pmatrix} \\mathbf{Z}^{(k+1)}_{\\text{row}} \\\\ \\mathbf{Z}^{(k+1)}_{\\text{col}} \\end{pmatrix} &= \\begin{pmatrix} \\mathbf{Z}^{(k)}_{\\text{row}} + \\rho(\\mathbf{D}_{\\text{row}}\\mathbf{U}^{(k+1)} - \\mathbf{V}^{(k+1)}_{\\text{row}}) \\\\ \\mathbf{Z}^{(k)}_{\\text{col}} + \\rho(\\mathbf{U}^{(k+1)}\\mathbf{D}_{\\text{col}} - \\mathbf{V}^{(k+1)}_{\\text{col}})\n\\end{pmatrix}\n\\end{align*}\\] This method has slightly worse per iteration performance, but has superior wall clock performance and better computational complexity.\nPer Iteration PerformanceWall Clock PerformanceThe generalized ADMM is used internally in my clustRviz software for both the exact solver and the CBASS pathwise algorithm.\nRelated Software: clustRviz\nCitation:\n@INPROCEEDINGS{Weylandt:2019-BiClustering,\n  TITLE=\"Splitting Methods for Convex Bi-Clustering and Co-Clustering\",\n  AUTHOR=\"Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755599\",\n  PAGES={237-244}\n}\n\n@PROCEEDINGS{DSW:2019,\n  TITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T18:01:46-06:00"
    },
    {
      "path": "publication_conga.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:46-06:00"
    },
    {
      "path": "publication_convex_wavelet_clustering.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:46-06:00"
    },
    {
      "path": "publication_fair_pca.html",
      "title": "Debiasing Projections for Fair Principal Components Analysis",
      "description": "Submitted Conference Publication",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        }
      ],
      "contents": "\nAbstract: Machine learning models can often reflect or exacerbate societal biases present in training data. While most attention in the nascent field of algorithmic fairness has focused on bias mitigation for supervised learning, biases can also be a problem for unsupervised analyses. With Principal Components Analysis (PCA), for example, biased dimension reduction can affect downstream analysis as well as affect data exploration and modeling decisions. In this paper, we propose a novel framework for Fair PCA that finds debiasing projections of the data, or projections that remove the effect from any protected attribute. We show that our approach has many advantages over existing formulations of Fair PCA including a fast, closed form solution, generality and wide applicability to many protect attributes, and its ease of interpretation and usage like projections in classical PCA. We demonstrate the bias mitigating advantages of our Fair PCA method compared to existing proposals for Fair PCA on several synthetic and benchmark datasets.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:46-06:00"
    },
    {
      "path": "publication_hepatoscore14.html",
      "title": "HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk",
      "description": "*Hepatology* 73(6), pp. 2278-2292. 2021.",
      "author": [
        {
          "name": "J. S. Morris et al.",
          "url": {}
        }
      ],
      "contents": "\nAbstract: Background and Aims: Therapeutic, clinical trial entry and stratification decisions for hepatocellular carcinoma (HCC) are made based on prognostic assessments, using clinical staging systems based on small numbers of empirically selected variables that insufficiently account for differences in biological characteristics of individual patients’ disease.\nApproach and Results: We propose an approach for constructing risk scores from circulating biomarkers that produce a global biological characterization of individual patient’s disease. Plasma samples were collected prospectively from 767 patients with HCC and 200 controls, and 317 proteins were quantified in a Clinical Laboratory Improvement Amendments–certified biomarker testing laboratory. We constructed a circulating biomarker aberration score for each patient, a score between 0 and 1 that measures the degree of aberration of his or her biomarker panel relative to normal, which we call HepatoScore. We used log-rank tests to assess its ability to substratify patients within existing staging systems/prognostic factors. To enhance clinical application, we constructed a single-sample score, HepatoScore-14, which requires only a subset of 14 representative proteins encompassing the global biological effects. Patients with HCC were split into three distinct groups (low, medium, and high HepatoScore) with vastly different prognoses (medial overall survival 38.2/18.3/7.1 months; P < 0.0001). Furthermore, HepatoScore accurately substratified patients within levels of existing prognostic factors and staging systems (P < 0.0001 for nearly all), providing substantial and sometimes dramatic refinement of expected patient outcomes with strong therapeutic implications. These results were recapitulated by HepatoScore-14, rigorously validated in repeated training/test splits, concordant across Myriad RBM (Austin, TX) and enzyme-linked immunosorbent assay kits, and established as an independent prognostic factor.\nConclusions: HepatoScore-14 augments existing HCC staging systems, dramatically refining patient prognostic assessments and therapeutic decision making and enrollment in clinical trials. The underlying strategy provides a global biological characterization of disease, and can be applied broadly to other disease settings and biological media.\nPublisher DOI: 10.1002/hep.31555\nPubMed: 32931023\nCitation:\n@ARTICLE{Moarris:2021,\n  AUTHOR=\"J.S. Morris and M.M. Hassan and Y.E. Zohner and Z. Wang and L. Xiao and A. Rashid and A. Haque and R. Abdel-Wahad and Y.A. Mohamed and K.L. Ballard and R.A. Wolff and B. George and L. Li and G.I. Allen and M. Weylandt and D. Li and W. Wang and K. Raghav and J. Yao and H.M. Amin and A.O. Kaseb\",\n  TITLE=\"HepatoScore14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk\",\n  JOURNAL=\"Hepatology\",\n  VOLUME=73,\n  NUMBER=6,\n  PAGES=\"2278--2292\",\n  YEAR=2021,\n  DOI=\"10.1002/hep.31555\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T18:01:47-06:00"
    },
    {
      "path": "publication_mrsfpca.html",
      "title": "Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques",
      "description": "*CAMSAP 2019: Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing*, pp.500-504. 2019.",
      "author": [
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: We consider the problem of estimating multiple principal components using the recently-proposed Sparse and Functional Principal Components Analysis (SFPCA) estimator. We first propose an extension of SFPCA which estimates several principal components simultaneously using manifold optimization techniques to enforce orthogonality constraints. While effective, this approach is computationally burdensome so we also consider iterative deflation approaches which take advantage of existing fast algorithms for rank-one SFPCA. We show that alternative deflation schemes can more efficiently extract signal from the data, in turn improving estimation of subsequent components. Finally, we compare the performance of our manifold optimization and deflation techniques in a scenario where orthogonality does not hold and find that they still lead to significantly improved performance.\nPublisher DOI: 10.1109/CAMSAP45676.2019.9022486\nWorking Copy: ArXiv 1907.12012\nSummary: This work extends my previous work on Sparse and Functional PCA to the case of estimating multiple principal components. While classical PCA gives nicely orthogonal and simultaneously estimable1 components, the situation for sparse PCA is less compelling. I translate the rank-1 SFPCA problem to a rank-\\(k\\) optimization problem over a pair of generalized Stiefel manifolds: \\[\\text{arg max}_{\\mathbf{U} \\in \\mathcal{V}_{n\\times k}^{\\mathbf{S}_u}, \\mathbf{V} \\in \\mathcal{V}_{p \\times k}^{\\mathbf{S}_v}} \\text{Tr}(\\mathbf{U}^T\\mathbf{X}\\mathbf{V}) - \\lambda_{\\mathbf{U}} P_{\\mathbf{U}}(\\mathbf{U}) - \\lambda_{\\mathbf{V}} P_{\\mathbf{V}}(\\mathbf{V})\\] To solve this problem, I take advantage recent non-smooth manifold-constrained optimization schemes2 and show that superior solutions can be obtained for reasonable sized-problems. (These methods require one or more matrix decompositions at each iteration, so they don’t scale to very large data.) While directly solving the rank-\\(k\\) problem is best, I also propose some improvements to the greedy rank-1 strategy that guarantee weaker forms of orthogonality. These improvements cannot impose true orthogonality, but they do guarantee that the estimated principal components are “residual-orthogonal” which ultimately improves statistical performance:\nMethod\nTwo-Way Orthogonality \\(u^TX_tv = 0\\)\nOne-Way Orthogonality \\(u^TX_t, X_tv = 0\\)\nSubsequent Orthogonality \\(u^TX_{t + s}, X_{t + s}v = 0\\) for all \\(s\\)\nRobust to Scale\nHotelling Deflation\n✓\nX\nX\nX\nProjection Deflation\n✓\n✓\nX\nX\nSchur Deflation\n✓\n✓\n✓\n✓\nInterestingly, all these forms of orthogonality tend to give improved performance, even when the underlying signal is not orthogonal.\nComparison of Multi-Rank SFPCA with Traditional SVD PCARelated Software: MoMA\nCitation:\n@INPROCEEDINGS{Weylandt:2019-MRSFPCA,\n  TITLE=\"Multi-Rank Sparse and Functional {PCA}: Manifold Optimization and Iterative Deflation Techniques\",\n  AUTHOR=\"Michael Weylandt\",\n  CROSSREF={CAMSAP:2019},\n  DOI=\"10.1109/CAMSAP45676.2019.9022486\",\n  PAGES={500-504}\n}\n\n@PROCEEDINGS{CAMSAP:2019,\n  TITLE=\"{CAMSAP} 2019: Proceedings of the 8\\textsuperscript{th} {IEEE} Workshop on Computational Advances in Multi-Sensor Adaptive Processing\",\n  YEAR=2019,\n  LOCATION=\"Le Gosier, Guadaloupe\",\n  EDITOR=\"Geert Leus and Antonio G. Marques\"\n}\nTaking the eigendecomposition as an “atomic” operation↩\nUnpublished as of 2019: now published as\n- S. Chen, S. Ma, A. M.-C. So, and T. Zhang, “Proximal gradient method for nonsmooth optimization over the Stiefel manifold,” SIAM Journal on Optimization, vol. 30, no. 1, pp. 210–239, 2020.\n- S. Chen, S. Ma, L. Xue, and H. Zou, “An alternating manifold proximal gradient method for sparse principal component analysis and sparse canonical correlation analysis,” INFORMS Journal on Optimization, vol. 2, no. 3, pp. 192– 208, 2020.\n\n↩\n",
      "last_modified": "2022-02-15T18:01:47-06:00"
    },
    {
      "path": "publication_network_convex_clustering.html",
      "title": "Network Clustering for Latent State and Changepoint Detection",
      "description": "Submitted Conference Publication",
      "author": [
        {
          "name": "Madeline Navarro",
          "url": {}
        },
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        },
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Network models provide a powerful and flexible framework for analyzing a wide range of structured data sources. In many situations of interest, however, multiple networks can be constructed to capture different aspects of an underlying phenomenon or to capture changing behavior over time. In such settings, it is often useful to  together related networks in attempt to identify patterns of common structure. In this paper, we propose a convex approach for the task of network clustering. Our approach uses a convex fusion penalty to induce a smoothly-varying tree-like cluster structure, eliminating the need to select the number of clusters . We provide an efficient algorithm for convex network clustering and demonstrate its effectiveness on synthetic examples.\nWorking Copy: ArXiv 2111.01273\nSummary: We consider the problem of clustering a set of networks. We “stack” these networks into a tensor format and then re-rexpress the network clustering problem as a tensor co-clustering problem. We solve this tensor co-clustering problem using a variant of convex (co-)clustering and provide an efficient algorithm for the resulting optimization problem.\nSchematic Diagram of Convex Network Clustering\n\n\n",
      "last_modified": "2022-02-15T18:01:47-06:00"
    },
    {
      "path": "publication_ng_volatility.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:47-06:00"
    },
    {
      "path": "publication_sfpca.html",
      "title": "Sparse and Functional Principal Components Analysis",
      "description": "*DSW 2019: Proceedings of the IEEE Data Science Workshop 2019*, pp.11-16. 2019.",
      "author": [
        {
          "name": "Genevera I. Allen",
          "url": "https://genevera.rice.edu"
        },
        {
          "name": "Michael Weylandt",
          "url": "https://michaelweylandt.github.io"
        }
      ],
      "contents": "\nAbstract: Regularized variants of Principal Components Analysis, especially Sparse PCA and Functional PCA, are among the most useful tools for the analysis of complex high-dimensional data. Many examples of massive data, have both sparse and functional (smooth) aspects and may benefit from a regularization scheme that can capture both forms of structure. For example, in neuro-imaging data, the brain’s response to a stimulus may be restricted to a discrete region of activation (spatial sparsity), while exhibiting a smooth response within that region. We propose a unified approach to regularized PCA which can induce both sparsity and smoothness in both the row and column principal components. Our framework generalizes much of the previous literature, with sparse, functional, two-way sparse, and two-way functional PCA all being special cases of our approach. Our method permits flexible combinations of sparsity and smoothness that lead to improvements in feature selection and signal recovery, as well as more interpretable PCA factors. We demonstrate the efficacy of our method on simulated data and a neuroimaging example on EEG data.\nPublisher DOI: 10.1109/DSW.2019.8755778\nWorking Copy: ArXiv 1309.2895\nSummary: We give a unified proposal for regularized PCA, allowing for both sparsity and smoothness (functional structure) in both the estimated observation and feature embeddings (left and right singular values): \\[\\text{arg max}_{\\mathbf{u} \\in \\overline{\\mathbb{B}}^n_{\\mathbf{S}_{\\mathbf{u}}}, \\mathbf{v} \\in \\overline{\\mathbb{B}}^p_{\\mathbf{S}_{\\mathbf{v}}}} \\mathbf{u}^T\\mathbf{X}\\mathbf{v} - \\lambda_{\\mathbf{u}} P_{\\mathbf{u}}(\\mathbf{u}) - \\lambda_{\\mathbf{v}} P_{\\mathbf{v}}(\\mathbf{v})\\] Our approach incorporates a variety of prior proposals as special cases, including:\nClassical (Unregularized) PCA\nOne-Way Sparse PCA: Shen and Huang (J. Multivariate Analysis, 2008)\nTwo-Way Sparse PCA: Allen, Grosenick, and Taylor (J. American Statistical Association, 2014) and Witten, Tibshirani, and Hastie (Biostatistics, 2009)\nOne-Way Functional PCA: Silverman (Annals of Statistics, 1996) and Huan, Shen, and Buja (Electronic Journal of Statistics, 2008)\nTwo-Way Functional PCA: Huang, Shen, and Buja (J. American Statistical Association, 2009)\nWe prove that our proposal is well-formulated, i.e., that all constraints are either binding or clearly slack with no masking, and give an efficient algorithm for finding a Nash point of the non-convex SFPCA problem. Finally, we apply our result in simulations and to EEG data and show that it finds meaningful and useful principal components.\nThe ArXiv-only appendices of this paper have quite a lot of useful content, including detailed analysis of the SFPCA problem and equivalencies to other regularized PCA schemes. Appendix C gives a thorough review of the regularized PCA literature as of 2019.\nIn Silico Comparison of SFPCA with other PCA VariantsRecovering space-time localized left and right singular vectors with SFPCARelated Software: MoMA\nCitation:\n@INPROCEEDINGS{Allen:2019,\n  TITLE=\"Sparse and Functional Principal Components Analysis\",\n  AUTHOR=\"Genevera I. Allen and Michael Weylandt\",\n  CROSSREF={DSW:2019},\n  DOI=\"10.1109/DSW.2019.8755778\",\n  PAGES={11-16}\n}\n\n@PROCEEDINGS{DSW:2019,\n  TITLE=\"{DSW} 2019: Proceedings of the 2\\textsuperscript{nd} {IEEE} Data Science Workshop\",\n  YEAR=2019,\n  EDITOR=\"George Karypis and George Michailidis and Rebecca Willett\",\n  PUBLISHER=\"{IEEE}\",\n  LOCATION=\"Minneapolis, Minnesota\"\n}\n\n\n\n",
      "last_modified": "2022-02-15T18:01:48-06:00"
    },
    {
      "path": "publication_tree_reproduction.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:48-06:00"
    },
    {
      "path": "publication_trout.html",
      "title": "Untitled",
      "description": "A new article created using the Distill format.\n",
      "author": [
        {
          "name": "Nora Jones",
          "url": "https://example.com/norajones"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:48-06:00"
    },
    {
      "path": "publications.html",
      "title": "Publications",
      "author": [],
      "contents": "\nBelow is a list of my publications: for more details on each publication, click for details.\nSubmitted\nM. Weylandt and G. Michailidis. “Coupled CP Decomposition for Principal Components Analysis of Symmetric Networks”\nM. Weylandt and G. I. Allen. “Debiasing Projections for Fair Principal Components Analysis”\nM. Navarro, G. I. Allen, and M. Weylandt. “Network Clustering for Latent State and Changepoint Detection”\nM. Weylandt, Y. Han, and K. B. Ensor. “Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating Futures Market Realized Volatility”\nWinner of the ASA Section on Business and Economic Statistics (B&E) 2020 Student Paper Competition\nA.P. Drager, M. Weylandt, G. Chuyong, D. Kenfack, D.W. Thomas, and A.E. Dunham. “Ecological correlates of reproductive status in a guild of Afrotropical understory trees.”\nPublished\nM. Weylandt, G. Michailidis, and T. M. Roddenberry. “Sparse Partial Least Squares for Coarse Noisy Graph Alignment.” SSP 2021: Proceedings of the 2021 IEEE Statistical Signal Processing Workshop 2021, pp.561-565. 2021.\nM. Weylandt, T.M. Roddenberry, and G. I. Allen. “Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering.” DSLW 2021: Proceedings of the IEEE Data Science and Learning Workshop 2021. pp.1-8. 2021.\nM. Weylandt and G. Michailidis. “Automatic Registration and Convex Clustering of Time Series.” ICASSP 2021: Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, pp.5609-5613. 2021.\nJ.S. Morris, M.M. Hassan, Y.E. Zohner, Z. Wang, L. Xiao, A. Rashid, A. Haque, R. Abdel-Wahad, Y.A. Mohamed, K.L. Ballard, R.A. Wolff, B. George, L. Li, G. I. Allen, M. Weylandt, D. Li, W. Wang, K. Raghav, J. Yao, H.M. Amin, and A.O. Kaseb. “HepatoScore‐14: Measures of biological heterogeneity significantly improve prediction of hepatocellular carcinoma risk.” Hepatology 73(6), pp. 2278-2292. 2021.\nM. Weylandt. “Multi-Rank Sparse and Functional PCA: Manifold Optimization and Iterative Deflation Techniques.” CAMSAP 2019: Proceedings of the IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pp.500-504. 2019.\nM. Weylandt, J. Nagorski, and G. I. Allen. “Interactive Visualizations and Fast Computation for Convex Clustering via Algorithmic Regularization.” Journal of Computational and Graphical Statistics 29(1), pp. 87-96. 2020.\nWinner of the ASA Section on Statistical Learning and Data Science (SLDS) 2019 Student Paper Competition\nM. Weylandt. “Splitting Methods for Convex Bi-Clustering and Co-Clustering.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.237-244. 2019.\nG. I. Allen and M. Weylandt. “Sparse and Functional Principal Components Analysis.” DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.11-16. 2019.\n\n\n\n",
      "last_modified": "2022-02-15T18:01:48-06:00"
    },
    {
      "path": "software.html",
      "title": "Michael Weylandt",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-02-15T18:01:49-06:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
